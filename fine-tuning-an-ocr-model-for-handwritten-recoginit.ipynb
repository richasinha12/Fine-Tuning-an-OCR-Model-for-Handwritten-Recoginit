{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile handwriting_ocr_finetuning.ipynb\n# %% [markdown]\n# # Handwriting OCR Fine-Tuning with TrOCR\n# ## Complete Pipeline with Kaggle Hardware Optimization","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\n# Install dependencies\n!pip install -q datasets transformers[torch] evaluate jiwer torchvision opencv-python-headless\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:30:11.516591Z","iopub.execute_input":"2025-04-13T16:30:11.516937Z","iopub.status.idle":"2025-04-13T16:31:25.625775Z","shell.execute_reply.started":"2025-04-13T16:30:11.516913Z","shell.execute_reply":"2025-04-13T16:31:25.625037Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:31:57.045584Z","iopub.execute_input":"2025-04-13T16:31:57.045826Z","iopub.status.idle":"2025-04-13T16:32:08.492323Z","shell.execute_reply.started":"2025-04-13T16:31:57.045807Z","shell.execute_reply":"2025-04-13T16:32:08.491430Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.51.2-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\nSuccessfully installed transformers-4.51.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%script echo skipping\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:32:44.330138Z","iopub.execute_input":"2025-04-13T16:32:44.330744Z","iopub.status.idle":"2025-04-13T16:32:44.344273Z","shell.execute_reply.started":"2025-04-13T16:32:44.330718Z","shell.execute_reply":"2025-04-13T16:32:44.343706Z"}},"outputs":[{"name":"stdout","text":"skipping\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# %% [code]\nimport os\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom datasets import load_dataset, DatasetDict, concatenate_datasets\nfrom transformers import (\n    TrOCRProcessor,\n    VisionEncoderDecoderModel,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    default_data_collator,\n    EarlyStoppingCallback\n)\nimport evaluate\nfrom torch.nn import DataParallel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:33:02.565022Z","iopub.execute_input":"2025-04-13T16:33:02.565670Z","iopub.status.idle":"2025-04-13T16:33:26.890279Z","shell.execute_reply.started":"2025-04-13T16:33:02.565647Z","shell.execute_reply":"2025-04-13T16:33:26.889756Z"}},"outputs":[{"name":"stderr","text":"2025-04-13 16:33:12.249061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744561992.449138      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744561992.506510      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Hardware-aware configuration\nNUM_GPUS = torch.cuda.device_count()\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:33:46.891673Z","iopub.execute_input":"2025-04-13T16:33:46.892560Z","iopub.status.idle":"2025-04-13T16:33:46.896371Z","shell.execute_reply.started":"2025-04-13T16:33:46.892532Z","shell.execute_reply":"2025-04-13T16:33:46.895613Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Training parameters\nMODEL_NAME = \"microsoft/trocr-large-handwritten\"\nMAX_EPOCHS = 10\nLEARNING_RATE = 5e-5\nIMAGE_SIZE = (384, 384)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:33:58.153908Z","iopub.execute_input":"2025-04-13T16:33:58.154150Z","iopub.status.idle":"2025-04-13T16:33:58.157798Z","shell.execute_reply.started":"2025-04-13T16:33:58.154135Z","shell.execute_reply":"2025-04-13T16:33:58.157144Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Batch size configuration (4 for P100, 8 for T4s)\nBATCH_SIZE = 8 if \"T4\" in torch.cuda.get_device_name(0) else 4\nBATCH_SIZE *= NUM_GPUS  # Scale with multiple GPUs\nGRADIENT_ACCUMULATION_STEPS = 2 if NUM_GPUS == 1 else 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:34:10.627267Z","iopub.execute_input":"2025-04-13T16:34:10.627810Z","iopub.status.idle":"2025-04-13T16:34:10.631407Z","shell.execute_reply.started":"2025-04-13T16:34:10.627785Z","shell.execute_reply":"2025-04-13T16:34:10.630681Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, DatasetDict\n\nclass KaggleDataLoader:\n    \"\"\"Handles dataset loading from Hugging Face\"\"\"\n    \n    @staticmethod\n    def load_iam():\n        \"\"\"Load IAM Handwriting from Hugging Face\"\"\"\n        # Load the IAM dataset from Hugging Face\n        return load_dataset(\"Teklia/IAM-line\")\n    \n    @staticmethod\n    def load_imgur5k():\n        \"\"\"Load Imgur5K from Hugging Face\"\"\"\n        # Load the Imgur5K dataset from Hugging Face\n        return load_dataset(\"staghado/IMGUR-dataset\")\n    \n    @staticmethod\n    def create_datasets():\n        \"\"\"Create combined dataset with validation split\"\"\"\n        # Load IAM and Imgur5K datasets\n        iam = KaggleDataLoader.load_iam()\n        imgur = KaggleDataLoader.load_imgur5k()\n        \n        # Combine training data and split into train/validation sets\n        combined_train = concatenate_datasets([iam[\"train\"], imgur[\"train\"]])\n        train_val_split = combined_train.train_test_split(test_size=0.1)\n        \n        return DatasetDict({\n            \"train\": train_val_split[\"train\"],\n            \"validation\": train_val_split[\"test\"],\n            \"test\": iam[\"test\"]  # Use IAM's test set as the test set\n        })\n\n# Initialize datasets\ndataset = KaggleDataLoader.create_datasets()\n\n# Print information about the dataset splits\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:34:14.189345Z","iopub.execute_input":"2025-04-13T16:34:14.189648Z","iopub.status.idle":"2025-04-13T16:40:40.179704Z","shell.execute_reply.started":"2025-04-13T16:34:14.189628Z","shell.execute_reply":"2025-04-13T16:40:40.179048Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81b84096914f48f2b263d4ea9245ddc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.parquet:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f35c6c262a546c79aa59e29368e0c8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.parquet:   0%|          | 0.00/24.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb93955461548b1afa5697d375ce815"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.parquet:   0%|          | 0.00/73.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e2789c426c54ed9bb9385b2f1d6c4a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/6482 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3431e05dee8459e88f9787d62324fb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/976 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49b95d38e5b940bb8dbc8c99a2b449d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2915 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0865bc09829455c8acd34844dea8be4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IMGUR5K-Handwriting-Dataset.zip:   0%|          | 0.00/4.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41cc26bf6a0f4a429442c1a912e3b4af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_0.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ad62d95b204a8ea37b8441d67761ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_1.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"305b89f6e91f4815801138345cd1ad5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_2.zip:   0%|          | 0.00/2.49G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d3e0f69b65449a9cce29caefea9049"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_3.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c0f87ff0f2a484c908f326c5f99f0b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_4.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10493a379d0e4718a18d373ad8b1c206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_5.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e68b51486c444eb9a09b85d47e465fc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_6.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7b344f98a90428b8ee0549a8c1c8728"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_7.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734ec5562b3a455a9811a2e6da5f6f73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_8.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f996801a704741239315247d7251495b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_9.zip:   0%|          | 0.00/2.36G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9431dcca714e44bfa94f247829910d19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_info.zip:   0%|          | 0.00/15.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"915b56b1512147dda830c17fbd26726a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"imgur8k-dataset.zip:   0%|          | 0.00/4.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f0145fed7684983a1d59831f3a7ca05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c20029eee4b4b06b6d86811cbb3e1cd"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['image', 'text'],\n        num_rows: 737146\n    })\n    validation: Dataset({\n        features: ['image', 'text'],\n        num_rows: 81906\n    })\n    test: Dataset({\n        features: ['image', 'text'],\n        num_rows: 2915\n    })\n})\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import TrOCRProcessor\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\nclass OCRPreprocessor:\n    \"\"\"Production-grade preprocessing with GPU-aware optimizations\"\"\"\n    \n    def __init__(self, model_name, image_size=(224, 224), aug_prob=0.5):\n        self.processor = TrOCRProcessor.from_pretrained(model_name)\n        self.image_size = image_size\n        self.aug_prob = aug_prob\n        \n    def _process_image(self, img):\n        \"\"\"Full image processing pipeline\"\"\"\n        # Convert to grayscale if not already\n        if img.mode != \"L\":\n            img = img.convert(\"L\")\n        img = np.array(img)\n        \n        # Noise reduction\n        img = cv2.fastNlMeansDenoising(img, h=10)\n        img = cv2.medianBlur(img, 3)\n        \n        # Adaptive thresholding\n        img = cv2.adaptiveThreshold(\n            img, 255,\n            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n            cv2.THRESH_BINARY, 11, 2\n        )\n        \n        # Augmentation\n        if np.random.rand() < self.aug_prob:\n            img = self._perspective_augmentation(img)\n            \n        # Resize to the desired image size\n        img = cv2.resize(img, self.image_size)\n        return img\n    \n    def _perspective_augmentation(self, img):\n        \"\"\"Add perspective variation\"\"\"\n        h, w = img.shape\n        pts1 = np.float32([[0, 0], [w, 0], [0, h], [w, h]])  # Source points\n        # Add random uniform perturbations to the destination points\n        pts2 = np.float32([\n            [np.random.uniform(-w * 0.05, w * 0.05), np.random.uniform(-h * 0.05, h * 0.05)],\n            [w + np.random.uniform(-w * 0.05, w * 0.05), np.random.uniform(-h * 0.05, h * 0.05)],\n            [np.random.uniform(-w * 0.05, w * 0.05), h + np.random.uniform(-h * 0.05, h * 0.05)],\n            [w + np.random.uniform(-w * 0.05, w * 0.05), h + np.random.uniform(-h * 0.05, h * 0.05)]\n        ])  # Destination points\n        M = cv2.getPerspectiveTransform(pts1, pts2)  # Perspective transformation matrix\n        return cv2.warpPerspective(img, M, (w, h))  # Apply the perspective warp\n    \n    def process_batch(self, examples):\n        \"\"\"Batch processing for dataset\"\"\"\n        processed_images = []\n\n        for img_data in examples[\"image\"]:\n            if isinstance(img_data, str):  # If the image is a path\n                img = Image.open(img_data)\n            else:  # If the image is raw data\n                img = img_data\n            \n            # Process the image\n            processed_img = Image.fromarray(self._process_image(img)).convert(\"RGB\")\n            processed_images.append(processed_img)\n        \n        # Ensure texts are strings\n        texts = [\n            str(text) if text is not None else \"\"  # Convert to string or use an empty string if None\n            for text in examples[\"text\"]\n        ]\n        \n        # Processor operations\n        pixel_values = self.processor(processed_images, return_tensors=\"pt\").pixel_values\n        labels = self.processor.tokenizer(\n            texts, \n            padding=\"max_length\", \n            max_length=64, \n            return_tensors=\"pt\"\n        ).input_ids\n        \n        return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n# Subset the 'train' split of the dataset to 500 examples\nsubset_size = 500\ntrain_dataset = dataset[\"train\"].shuffle(seed=42).select(range(subset_size))  # Take first 500 examples of the train split\n\n# Update the subset dataset with preprocessing\nMODEL_NAME = \"microsoft/trocr-base-handwritten\"  # Example model name\nIMAGE_SIZE = (224, 224)\n\npreprocessor = OCRPreprocessor(model_name=MODEL_NAME, image_size=IMAGE_SIZE)\nprocessed_dataset = train_dataset.map(\n    preprocessor.process_batch,\n    batched=True,\n    batch_size=16,  # Smaller batch size to reduce memory usage\n    remove_columns=[\"image\", \"text\"]  # Remove unused columns\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:53:48.862982Z","iopub.execute_input":"2025-04-13T16:53:48.863788Z","iopub.status.idle":"2025-04-13T17:02:08.291700Z","shell.execute_reply.started":"2025-04-13T16:53:48.863764Z","shell.execute_reply":"2025-04-13T17:02:08.291076Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a9309fb495b4b359e089e0280b40cf4"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d153a0162b941958eef3f2babc2ba88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d740a9ed84774247bd91081f3275933f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d80929ab47e42ae85072702df7aad93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cdc588d21b94d33aa43ba7bb0469537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b4f8fbe787d49039c33b77268f4553f"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"print(processed_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:03:37.971244Z","iopub.execute_input":"2025-04-13T17:03:37.971555Z","iopub.status.idle":"2025-04-13T17:03:37.975454Z","shell.execute_reply.started":"2025-04-13T17:03:37.971535Z","shell.execute_reply":"2025-04-13T17:03:37.974631Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['pixel_values', 'labels'],\n    num_rows: 500\n})\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n\n# Initialize processor\nMODEL_NAME = \"microsoft/trocr-base-handwritten\"  # Example model name\nprocessor = TrOCRProcessor.from_pretrained(MODEL_NAME)\n\n# Initialize model with DataParallel\nmodel = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:07:51.810495Z","iopub.execute_input":"2025-04-13T17:07:51.810772Z","iopub.status.idle":"2025-04-13T17:07:52.822013Z","shell.execute_reply.started":"2025-04-13T17:07:51.810756Z","shell.execute_reply":"2025-04-13T17:07:52.821278Z"}},"outputs":[{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.2\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.2\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"if NUM_GPUS > 1:\n    model = DataParallel(model)\nmodel.to(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:08:42.822285Z","iopub.execute_input":"2025-04-13T17:08:42.822980Z","iopub.status.idle":"2025-04-13T17:08:43.581534Z","shell.execute_reply.started":"2025-04-13T17:08:42.822955Z","shell.execute_reply":"2025-04-13T17:08:43.580908Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): VisionEncoderDecoderModel(\n    (encoder): ViTModel(\n      (embeddings): ViTEmbeddings(\n        (patch_embeddings): ViTPatchEmbeddings(\n          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): ViTEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x ViTLayer(\n            (attention): ViTAttention(\n              (attention): ViTSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=False)\n                (key): Linear(in_features=768, out_features=768, bias=False)\n                (value): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (output): ViTSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): ViTIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): ViTOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n        )\n      )\n      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (pooler): ViTPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (decoder): TrOCRForCausalLM(\n      (model): TrOCRDecoderWrapper(\n        (decoder): TrOCRDecoder(\n          (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n          (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (layers): ModuleList(\n            (0-11): 12 x TrOCRDecoderLayer(\n              (self_attn): TrOCRAttention(\n                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (activation_fn): GELUActivation()\n              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (encoder_attn): TrOCRAttention(\n                (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n                (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n      (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Split into train and validation sets\ntrain_test_split = processed_dataset.train_test_split(test_size=0.2, seed=42)  # 80% train, 20% validation\ntrain_dataset = train_test_split[\"train\"]\nvalidation_dataset = train_test_split[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:09:45.896569Z","iopub.execute_input":"2025-04-13T17:09:45.896938Z","iopub.status.idle":"2025-04-13T17:09:45.906245Z","shell.execute_reply.started":"2025-04-13T17:09:45.896908Z","shell.execute_reply":"2025-04-13T17:09:45.905432Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=MAX_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE//NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE//NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    fp16=True,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    report_to=\"none\",\n    predict_with_generate=True,\n    generation_max_length=64,\n    metric_for_best_model=\"cer\",\n    load_best_model_at_end=True,\n    greater_is_better=False,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    dataloader_num_workers=4*NUM_GPUS,\n    remove_unused_columns=False,  # Prevent trainer from dropping columns\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:09:49.356997Z","iopub.execute_input":"2025-04-13T17:09:49.357534Z","iopub.status.idle":"2025-04-13T17:09:49.393828Z","shell.execute_reply.started":"2025-04-13T17:09:49.357513Z","shell.execute_reply":"2025-04-13T17:09:49.393282Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:09:54.359506Z","iopub.execute_input":"2025-04-13T17:09:54.360496Z","iopub.status.idle":"2025-04-13T17:09:54.364002Z","shell.execute_reply.started":"2025-04-13T17:09:54.360469Z","shell.execute_reply":"2025-04-13T17:09:54.363308Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, EarlyStoppingCallback\n\n# Filter out empty labels from datasets\ndef filter_empty_labels(example):\n    return example[\"labels\"] is not None and len(example[\"labels\"]) > 0\n\ntrain_dataset = train_dataset.filter(filter_empty_labels)\nvalidation_dataset = validation_dataset.filter(filter_empty_labels)\n\n# Sanity check to ensure no empty labels\ntrain_dataset = train_dataset.filter(lambda example: len(example[\"labels\"]) > 0)\nvalidation_dataset = validation_dataset.filter(lambda example: len(example[\"labels\"]) > 0)\n\n# If the model is wrapped in DataParallel, access the underlying model\nif isinstance(model, torch.nn.DataParallel):\n    model = model.module\n\n# Update compute_metrics to handle empty predictions/references\ndef compute_metrics(pred):\n    predictions = pred.predictions\n    labels = pred.label_ids\n\n    # Decode predictions and labels\n    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Remove empty references and corresponding predictions\n    valid_pairs = [\n        (pred.strip(), label.strip())\n        for pred, label in zip(decoded_preds, decoded_labels)\n        if label.strip()  # Ensure reference is not empty\n    ]\n    decoded_preds, decoded_labels = zip(*valid_pairs) if valid_pairs else ([], [])\n\n    if not decoded_labels:  # If all references were empty, return a default metric\n        return {\"cer\": 1.0}\n\n    # Compute metrics (e.g., CER)\n    cer = calculate_cer(decoded_labels, decoded_preds)\n    return {\"cer\": cer}\n\n# Initialize trainer with early stopping\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  # Use the processed train split\n    eval_dataset=validation_dataset,  # Use the processed validation split\n    data_collator=default_data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n# Start training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:30:35.127927Z","iopub.execute_input":"2025-04-13T17:30:35.128261Z","iopub.status.idle":"2025-04-13T17:40:52.550632Z","shell.execute_reply.started":"2025-04-13T17:30:35.128236Z","shell.execute_reply":"2025-04-13T17:40:52.549753Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2bdcfcf215f4861a05d0e2d987ccc0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd341faca79746f09c8f03cc30313fbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a234e5000b423b9be1bd11ec1925b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55281fbd74994c5d93d1e88803c7457d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/250 06:30 < 09:58, 0.25 it/s, Epoch 4/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.103700</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['decoder.output_projection.weight'].\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=0.10371293067932129, metrics={'train_runtime': 417.8224, 'train_samples_per_second': 9.573, 'train_steps_per_second': 0.598, 'total_flos': 1.1972563055935488e+18, 'train_loss': 0.10371293067932129, 'epoch': 4.0})"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# Access the test dataset\ntest_dataset = dataset[\"test\"]\n\n# Preprocess the test dataset\nprocessed_test_dataset = test_dataset.map(\n    preprocessor.process_batch,\n    batched=True,\n    batch_size=16,\n    remove_columns=[\"image\", \"text\"]\n)\n\n#Final evaluation\n''''results = trainer.evaluate(processed_test_dataset, metric_key_prefix=\"test\")\nprint(f\"Final CER: {results['test_cer']*100:.2f}%\")\nprint(f\"Final WER: {results['test_wer']*100:.2f}%\")''''''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(pred):\n    predictions = pred.predictions\n    labels = pred.label_ids\n\n    # Decode predictions and labels\n    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Remove empty references and corresponding predictions\n    valid_pairs = [\n        (pred.strip(), label.strip())\n        for pred, label in zip(decoded_preds, decoded_labels)\n        if label.strip()  # Ensure reference is not empty\n    ]\n    decoded_preds, decoded_labels = zip(*valid_pairs) if valid_pairs else ([], [])\n\n    if not decoded_labels:  # If all references were empty, return default metrics\n        return {\"cer\": 1.0, \"wer\": 1.0}\n\n    # Compute metrics\n    cer_value = calculate_cer(decoded_labels, decoded_preds)\n    wer_value = cer_value  # For simplicity, you can use CER as a proxy for WER or use another method for WER\n    return {\"cer\": cer_value, \"wer\": wer_value}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:30:40.070866Z","iopub.execute_input":"2025-04-13T18:30:40.071350Z","iopub.status.idle":"2025-04-13T18:30:40.076784Z","shell.execute_reply.started":"2025-04-13T18:30:40.071327Z","shell.execute_reply":"2025-04-13T18:30:40.075963Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Final evaluation\nresults = trainer.evaluate(processed_test_dataset, metric_key_prefix=\"test\")\nprint(f\"Final CER: {results['test_cer']*100:.2f}%\")\nprint(f\"Final WER: {results['test_wer']*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:01:51.187727Z","iopub.status.idle":"2025-04-13T18:01:51.187976Z","shell.execute_reply.started":"2025-04-13T18:01:51.187873Z","shell.execute_reply":"2025-04-13T18:01:51.187883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model (handling DataParallel)\nmodel_to_save = model.module if hasattr(model, 'module') else model\ntorch.save(model_to_save.state_dict(), \"trocr_finetuned.pth\")\nmodel_to_save.save_pretrained(\"./final_model\")\nprocessor.save_pretrained(\"./final_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:26:19.115219Z","iopub.execute_input":"2025-04-13T18:26:19.115552Z","iopub.status.idle":"2025-04-13T18:26:34.031645Z","shell.execute_reply.started":"2025-04-13T18:26:19.115529Z","shell.execute_reply":"2025-04-13T18:26:34.030835Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"import os\nimport torch\n\n# Check current working directory\nprint(\"Current working directory:\", os.getcwd())\n\n# Check if final_model directory exists\nif os.path.exists(\"./final_model\"):\n    print(\"Directory './final_model' exists. Contents:\", os.listdir(\"./final_model\"))\nelse:\n    print(\"Directory './final_model' does not exist.\")\n\n# Check if .pth file exists\nif os.path.exists(\"trocr_finetuned.pth\"):\n    print(\"File 'trocr_finetuned.pth' exists.\")\nelse:\n    print(\"File 'trocr_finetuned.pth' does not exist.\")\n\n# Test write permissions\ntry:\n    with open(\"test_write_permissions.txt\", \"w\") as f:\n        f.write(\"Testing write permissions.\")\n    print(\"Write permissions are OK.\")\n    os.remove(\"test_write_permissions.txt\")\nexcept IOError:\n    print(\"No write permissions in the current directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:09:42.270155Z","iopub.execute_input":"2025-04-13T18:09:42.270671Z","iopub.status.idle":"2025-04-13T18:09:42.277123Z","shell.execute_reply.started":"2025-04-13T18:09:42.270646Z","shell.execute_reply":"2025-04-13T18:09:42.276497Z"}},"outputs":[{"name":"stdout","text":"Current working directory: /kaggle/working\nDirectory './final_model' exists. Contents: ['config.json', 'generation_config.json', 'special_tokens_map.json', 'tokenizer_config.json', 'preprocessor_config.json', 'merges.txt', 'vocab.json', 'tokenizer.json', 'model.safetensors']\nFile 'trocr_finetuned.pth' exists.\nWrite permissions are OK.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# %% [markdown]\n# ## Inference Pipeline\n\n# %% [code]\nclass ProductionOCR:\n    def __init__(self, model_path):\n        self.processor = TrOCRProcessor.from_pretrained(model_path)\n        self.model = VisionEncoderDecoderModel.from_pretrained(model_path).to(DEVICE)\n        \n    def predict(self, image_path):\n        image = Image.open(image_path).convert(\"RGB\")\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.to(DEVICE)\n        outputs = self.model.generate(pixel_values)\n        return self.processor.decode(outputs[0], skip_special_tokens=True)\n\n# Usage\nocr = ProductionOCR(\"./final_model\")\nprint(ocr.predict(\"/kaggle/input/iam-handwriting/test/a01-007-02.png\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}