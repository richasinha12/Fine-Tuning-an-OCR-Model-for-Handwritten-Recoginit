# -*- coding: utf-8 -*-
"""fine-tuning-an-ocr-model-for-handwrirecoginit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1repdW0JXmtUKWBDuSiKPM9-tVEJVI7rT

%%writefile handwriting_ocr_finetuning.ipynb
# %% [markdown]
# # Handwriting OCR Fine-Tuning with TrOCR
# ## Complete Pipeline with Kaggle Hardware Optimization
"""

# %% [code]
# Install dependencies
!pip install -q datasets transformers[torch] evaluate jiwer torchvision opencv-python-headless

!pip install --upgrade transformers

#%%script echo skipping
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# %% [code]
import os
import cv2
import torch
import numpy as np
from PIL import Image
from datasets import load_dataset, DatasetDict, concatenate_datasets
from transformers import (
    TrOCRProcessor,
    VisionEncoderDecoderModel,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    default_data_collator,
    EarlyStoppingCallback
)
import evaluate
from torch.nn import DataParallel

# Hardware-aware configuration
NUM_GPUS = torch.cuda.device_count()
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Training parameters
MODEL_NAME = "microsoft/trocr-large-handwritten"
MAX_EPOCHS = 10
LEARNING_RATE = 5e-5
IMAGE_SIZE = (384, 384)

# Batch size configuration (4 for P100, 8 for T4s)
BATCH_SIZE = 8 if "T4" in torch.cuda.get_device_name(0) else 4
BATCH_SIZE *= NUM_GPUS  # Scale with multiple GPUs
GRADIENT_ACCUMULATION_STEPS = 2 if NUM_GPUS == 1 else 1

from datasets import load_dataset, concatenate_datasets, DatasetDict

class KaggleDataLoader:
    """Handles dataset loading from Hugging Face"""

    @staticmethod
    def load_iam():
        """Load IAM Handwriting from Hugging Face"""
        # Load the IAM dataset from Hugging Face
        return load_dataset("Teklia/IAM-line")

    @staticmethod
    def load_imgur5k():
        """Load Imgur5K from Hugging Face"""
        # Load the Imgur5K dataset from Hugging Face
        return load_dataset("staghado/IMGUR-dataset")

    @staticmethod
    def create_datasets():
        """Create combined dataset with validation split"""
        # Load IAM and Imgur5K datasets
        iam = KaggleDataLoader.load_iam()
        imgur = KaggleDataLoader.load_imgur5k()

        # Combine training data and split into train/validation sets
        combined_train = concatenate_datasets([iam["train"], imgur["train"]])
        train_val_split = combined_train.train_test_split(test_size=0.1)

        return DatasetDict({
            "train": train_val_split["train"],
            "validation": train_val_split["test"],
            "test": iam["test"]  # Use IAM's test set as the test set
        })

# Initialize datasets
dataset = KaggleDataLoader.create_datasets()

# Print information about the dataset splits
print(dataset)

from transformers import TrOCRProcessor
from PIL import Image
import numpy as np
import cv2

class OCRPreprocessor:
    """Production-grade preprocessing with GPU-aware optimizations"""

    def __init__(self, model_name, image_size=(224, 224), aug_prob=0.5):
        self.processor = TrOCRProcessor.from_pretrained(model_name)
        self.image_size = image_size
        self.aug_prob = aug_prob

    def _process_image(self, img):
        """Full image processing pipeline"""
        # Convert to grayscale if not already
        if img.mode != "L":
            img = img.convert("L")
        img = np.array(img)

        # Noise reduction
        img = cv2.fastNlMeansDenoising(img, h=10)
        img = cv2.medianBlur(img, 3)

        # Adaptive thresholding
        img = cv2.adaptiveThreshold(
            img, 255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY, 11, 2
        )

        # Augmentation
        if np.random.rand() < self.aug_prob:
            img = self._perspective_augmentation(img)

        # Resize to the desired image size
        img = cv2.resize(img, self.image_size)
        return img

    def _perspective_augmentation(self, img):
        """Add perspective variation"""
        h, w = img.shape
        pts1 = np.float32([[0, 0], [w, 0], [0, h], [w, h]])  # Source points
        # Add random uniform perturbations to the destination points
        pts2 = np.float32([
            [np.random.uniform(-w * 0.05, w * 0.05), np.random.uniform(-h * 0.05, h * 0.05)],
            [w + np.random.uniform(-w * 0.05, w * 0.05), np.random.uniform(-h * 0.05, h * 0.05)],
            [np.random.uniform(-w * 0.05, w * 0.05), h + np.random.uniform(-h * 0.05, h * 0.05)],
            [w + np.random.uniform(-w * 0.05, w * 0.05), h + np.random.uniform(-h * 0.05, h * 0.05)]
        ])  # Destination points
        M = cv2.getPerspectiveTransform(pts1, pts2)  # Perspective transformation matrix
        return cv2.warpPerspective(img, M, (w, h))  # Apply the perspective warp

    def process_batch(self, examples):
        """Batch processing for dataset"""
        processed_images = []

        for img_data in examples["image"]:
            if isinstance(img_data, str):  # If the image is a path
                img = Image.open(img_data)
            else:  # If the image is raw data
                img = img_data

            # Process the image
            processed_img = Image.fromarray(self._process_image(img)).convert("RGB")
            processed_images.append(processed_img)

        # Ensure texts are strings
        texts = [
            str(text) if text is not None else ""  # Convert to string or use an empty string if None
            for text in examples["text"]
        ]

        # Processor operations
        pixel_values = self.processor(processed_images, return_tensors="pt").pixel_values
        labels = self.processor.tokenizer(
            texts,
            padding="max_length",
            max_length=64,
            return_tensors="pt"
        ).input_ids

        return {"pixel_values": pixel_values, "labels": labels}

# Subset the 'train' split of the dataset to 500 examples
subset_size = 500
train_dataset = dataset["train"].shuffle(seed=42).select(range(subset_size))  # Take first 500 examples of the train split

# Update the subset dataset with preprocessing
MODEL_NAME = "microsoft/trocr-base-handwritten"  # Example model name
IMAGE_SIZE = (224, 224)

preprocessor = OCRPreprocessor(model_name=MODEL_NAME, image_size=IMAGE_SIZE)
processed_dataset = train_dataset.map(
    preprocessor.process_batch,
    batched=True,
    batch_size=16,  # Smaller batch size to reduce memory usage
    remove_columns=["image", "text"]  # Remove unused columns
)

print(processed_dataset)

from transformers import TrOCRProcessor, VisionEncoderDecoderModel

# Initialize processor
MODEL_NAME = "microsoft/trocr-base-handwritten"  # Example model name
processor = TrOCRProcessor.from_pretrained(MODEL_NAME)

# Initialize model with DataParallel
model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id

if NUM_GPUS > 1:
    model = DataParallel(model)
model.to(DEVICE)

# Split into train and validation sets
train_test_split = processed_dataset.train_test_split(test_size=0.2, seed=42)  # 80% train, 20% validation
train_dataset = train_test_split["train"]
validation_dataset = train_test_split["test"]

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    num_train_epochs=MAX_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE//NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE//NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    fp16=True,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=100,
    report_to="none",
    predict_with_generate=True,
    generation_max_length=64,
    metric_for_best_model="cer",
    load_best_model_at_end=True,
    greater_is_better=False,
    warmup_ratio=0.1,
    weight_decay=0.01,
    dataloader_num_workers=4*NUM_GPUS,
    remove_unused_columns=False,  # Prevent trainer from dropping columns
)

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from transformers import Seq2SeqTrainer, EarlyStoppingCallback

# Filter out empty labels from datasets
def filter_empty_labels(example):
    return example["labels"] is not None and len(example["labels"]) > 0

train_dataset = train_dataset.filter(filter_empty_labels)
validation_dataset = validation_dataset.filter(filter_empty_labels)

# Sanity check to ensure no empty labels
train_dataset = train_dataset.filter(lambda example: len(example["labels"]) > 0)
validation_dataset = validation_dataset.filter(lambda example: len(example["labels"]) > 0)

# If the model is wrapped in DataParallel, access the underlying model
if isinstance(model, torch.nn.DataParallel):
    model = model.module

# Update compute_metrics to handle empty predictions/references
def compute_metrics(pred):
    predictions = pred.predictions
    labels = pred.label_ids

    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Filter out empty labels
    valid_pairs = [(p, l) for p, l in zip(decoded_preds, decoded_labels) if l.strip()]
    if not valid_pairs:
        return {"cer": 1.0, "wer": 1.0}

    decoded_preds, decoded_labels = zip(*valid_pairs)

    # Compute both metrics
    cer = calculate_cer(decoded_labels, decoded_preds)
    wer = calculate_wer(decoded_labels, decoded_preds)

    return {
        "cer": cer,
        "wer": wer,
        "eval_cer": cer,  # Required for early stopping
    }

# Initialize trainer with early stopping
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,  # Use the processed train split
    eval_dataset=validation_dataset,  # Use the processed validation split
    data_collator=default_data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# Start training
trainer.train()

import numpy as np
from jiwer import wer  # Install with: pip install jiwer

from datasets import load_dataset
from transformers import Trainer
import editdistance  # Ensure this library is installed
from sklearn.model_selection import train_test_split

# Define CER calculation
def calculate_cer(references, predictions):
    """
    Calculate the Character Error Rate (CER).
    CER = (Substitutions + Deletions + Insertions) / Total Characters in Reference
    """
    total_chars = 0
    total_errors = 0

    for ref, pred in zip(references, predictions):
        total_chars += len(ref)
        total_errors += editdistance.eval(ref, pred)

    return total_errors / total_chars if total_chars > 0 else 1.0

# Define WER calculation
def calculate_wer(references, predictions):
    """
    Calculate the Word Error Rate (WER).
    WER = (Substitutions + Deletions + Insertions) / Total Words in Reference
    """
    total_words = 0
    total_errors = 0

    for ref, pred in zip(references, predictions):
        ref_words = ref.split()
        pred_words = pred.split()
        total_words += len(ref_words)
        total_errors += editdistance.eval(ref_words, pred_words)

    return total_errors / total_words if total_words > 0 else 1.0

# Define metrics computation
def compute_metrics(pred):
    predictions = pred.predictions
    labels = pred.label_ids

    # Decode predictions and labels
    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Remove empty references and corresponding predictions
    valid_pairs = [
        (pred.strip(), label.strip())
        for pred, label in zip(decoded_preds, decoded_labels)
        if label.strip()  # Ensure reference is not empty
    ]
    decoded_preds, decoded_labels = zip(*valid_pairs) if valid_pairs else ([], [])

    if not decoded_labels:  # If all references were empty, return default metrics
        return {"cer": 1.0, "wer": 1.0}

    # Compute metrics
    cer_value = calculate_cer(decoded_labels, decoded_preds)
    wer_value = calculate_wer(decoded_labels, decoded_preds)
    return {"cer": cer_value, "wer": wer_value}

# Load the IAM dataset from Hugging Face
#iam_dataset = load_dataset("Teklia/IAM-line")

#iam["train"]

# Subset the test dataset to 500 examples (or any desired number)
# Note: Ensure that the test set is representative of the full test set
subset_test_dataset = iam["test"].select(range(500))  # Use 500 examples for evaluation

# Preprocess the test dataset
# Assuming preprocessor is defined in your project and processes the dataset
processed_test_dataset = subset_test_dataset.map(
    preprocessor.process_batch,
    batched=True,
    batch_size=16,
    remove_columns=["image", "text"]
)

# Evaluate the model on the subset test dataset
results = trainer.evaluate(processed_test_dataset, metric_key_prefix="test")

# Print results
print(f"Test CER (IAM dataset subset): {results['test_cer']*100:.2f}%")
print(f"Test WER (IAM dataset subset): {results['test_wer']*100:.2f}%")

# Check if the target CER and WER are met
if results['test_cer']*100 <= 7 and results['test_wer']*100 <= 15:
    print("Target achieved: CER ≤ 7% and WER ≤ 15%")
else:
    print("Target not achieved. Adjust training or preprocessing.")

'''''from datasets import load_dataset
from transformers import Trainer
import editdistance  # Ensure this library is installed
from sklearn.model_selection import train_test_split

# Define CER calculation
def calculate_cer(references, predictions):
    """
    Calculate the Character Error Rate (CER).
    CER = (Substitutions + Deletions + Insertions) / Total Characters in Reference
    """
    total_chars = 0
    total_errors = 0

    for ref, pred in zip(references, predictions):
        total_chars += len(ref)
        total_errors += editdistance.eval(ref, pred)

    return total_errors / total_chars if total_chars > 0 else 1.0

# Define WER calculation
def calculate_wer(references, predictions):
    """
    Calculate the Word Error Rate (WER).
    WER = (Substitutions + Deletions + Insertions) / Total Words in Reference
    """
    total_words = 0
    total_errors = 0

    for ref, pred in zip(references, predictions):
        ref_words = ref.split()
        pred_words = pred.split()
        total_words += len(ref_words)
        total_errors += editdistance.eval(ref_words, pred_words)

    return total_errors / total_words if total_words > 0 else 1.0

# Define metrics computation
def compute_metrics(pred):
    predictions = pred.predictions
    labels = pred.label_ids

    # Decode predictions and labels
    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Remove empty references and corresponding predictions
    valid_pairs = [
        (pred.strip(), label.strip())
        for pred, label in zip(decoded_preds, decoded_labels)
        if label.strip()  # Ensure reference is not empty
    ]
    decoded_preds, decoded_labels = zip(*valid_pairs) if valid_pairs else ([], [])

    if not decoded_labels:  # If all references were empty, return default metrics
        return {"cer": 1.0, "wer": 1.0}

    # Compute metrics
    cer_value = calculate_cer(decoded_labels, decoded_preds)
    wer_value = calculate_wer(decoded_labels, decoded_preds)
    return {"cer": cer_value, "wer": wer_value}

# Load the IAM dataset from Hugging Face
iam_dataset = load_dataset("Teklia/IAM-line")

# Subset the test dataset to 500 examples (or any desired number)
# Note: Ensure that the test set is representative of the full test set
subset_test_dataset = iam_dataset["test"].select(range(500))  # Use 500 examples for evaluation

# Preprocess the test dataset
# Assuming preprocessor is defined in your project and processes the dataset
processed_test_dataset = subset_test_dataset.map(
    preprocessor.process_batch,
    batched=True,
    batch_size=16,
    remove_columns=["image", "text"]
)

# Evaluate the model on the subset test dataset
results = trainer.evaluate(processed_test_dataset, metric_key_prefix="test")

# Print results
print(f"Test CER (IAM dataset subset): {results['test_cer']*100:.2f}%")
print(f"Test WER (IAM dataset subset): {results['test_wer']*100:.2f}%")

# Check if the target CER and WER are met
if results['test_cer']*100 <= 7 and results['test_wer']*100 <= 15:
    print("Target achieved: CER ≤ 7% and WER ≤ 15%")
else:
    print("Target not achieved. Adjust training or preprocessing.")''''''

# # Take small subsets (500 samples each)
# small_train = dataset["train"].shuffle(seed=42).select(range(500))
# small_val = dataset["validation"].shuffle(seed=42).select(range(500))
# small_test = dataset["test"].shuffle(seed=42).select(range(500))  # Note: test set only has 2915 total

# # Create new DatasetDict with small subsets
# small_dataset = DatasetDict({
#     "train": small_train,
#     "validation": small_val,
#     "test": small_test
# })

# ### 4. Evaluation on Small Test Set

# # Preprocess the small test set (500 samples)
# processed_test = small_dataset["test"].map(
#     preprocessor.process_batch,
#     batched=True,
#     batch_size=16,
#     remove_columns=["image", "text"]
# )

# import numpy as np
# from jiwer import wer  # Install with: pip install jiwer

# def calculate_wer(reference_texts, predicted_texts):
#     """
#     Robust WER calculation with input validation
#     """
#     # Convert to list if single string
#     if isinstance(reference_texts, str):
#         reference_texts = [reference_texts]
#     if isinstance(predicted_texts, str):
#         predicted_texts = [predicted_texts]

#     # Ensure inputs are clean strings
#     reference_texts = [str(text).strip() for text in reference_texts]
#     predicted_texts = [str(text).strip() for text in predicted_texts]

#     # Filter empty references
#     valid_pairs = [(pred, ref) for pred, ref in zip(predicted_texts, reference_texts) if ref]
#     if not valid_pairs:
#         return 1.0

#     preds, refs = zip(*valid_pairs)
#     return wer(list(refs), list(preds))

# def compute_metrics(pred):
#     predictions = pred.predictions
#     labels = pred.label_ids

#     # Decode with error handling
#     try:
#         decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)
#         decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)
#     except Exception as e:
#         print(f"Decoding error: {e}")
#         return {"cer": 1.0, "wer": 1.0}

#     # Ensure string type and strip whitespace
#     decoded_preds = [str(p).strip() for p in decoded_preds]
#     decoded_labels = [str(l).strip() for l in decoded_labels]

#     # Filter empty labels
#     valid_pairs = [(p, l) for p, l in zip(decoded_preds, decoded_labels) if l]
#     if not valid_pairs:
#         return {"cer": 1.0, "wer": 1.0, "eval_cer": 1.0}

#     decoded_preds, decoded_labels = zip(*valid_pairs)

#     # Compute metrics
#     cer = calculate_cer(decoded_labels, decoded_preds)
#     wer = calculate_wer(decoded_labels, decoded_preds)

#     return {
#         "cer": cer,
#         "wer": wer,
#         "eval_cer": cer,
#     }

# # Evaluation with proper error handling
# try:
#     results = trainer.evaluate(processed_test)
#     print(f"Test CER: {results.get('eval_cer', 1.0)*100:.2f}%")
#     print(f"Test WER: {results.get('eval_wer', 1.0)*100:.2f}%")
# except Exception as e:
#     print(f"Evaluation failed: {e}")
#     print("Default metrics (CER/WER=100%) due to evaluation error")

# results = trainer.evaluate(processed_test)
# print(f"Test CER: {results.get('eval_cer', 1.0)*100:.2f}%")
# print(f"Test WER: {results.get('eval_wer', 1.0)*100:.2f}%")

# # Save model (handling DataParallel)
# model_to_save = model.module if hasattr(model, 'module') else model
# torch.save(model_to_save.state_dict(), "trocr_finetuned.pth")
# model_to_save.save_pretrained("./final_model")
# processor.save_pretrained("./final_model")

# import os
# import torch

# # Check current working directory
# print("Current working directory:", os.getcwd())

# # Check if final_model directory exists
# if os.path.exists("./final_model"):
#     print("Directory './final_model' exists. Contents:", os.listdir("./final_model"))
# else:
#     print("Directory './final_model' does not exist.")

# # Check if .pth file exists
# if os.path.exists("trocr_finetuned.pth"):
#     print("File 'trocr_finetuned.pth' exists.")
# else:
#     print("File 'trocr_finetuned.pth' does not exist.")

# # Test write permissions
# try:
#     with open("test_write_permissions.txt", "w") as f:
#         f.write("Testing write permissions.")
#     print("Write permissions are OK.")
#     os.remove("test_write_permissions.txt")
# except IOError:
#     print("No write permissions in the current directory.")

# %% [markdown]
# # ## Inference Pipeline

# # %% [code]
# class ProductionOCR:
#     def __init__(self, model_path):
#         self.processor = TrOCRProcessor.from_pretrained(model_path)
#         self.model = VisionEncoderDecoderModel.from_pretrained(model_path).to(DEVICE)

#     def predict(self, image_path):
#         image = Image.open(image_path).convert("RGB")
#         pixel_values = self.processor(image, return_tensors="pt").pixel_values.to(DEVICE)
#         outputs = self.model.generate(pixel_values)
#         return self.processor.decode(outputs[0], skip_special_tokens=True)

# # Usage
# ocr = ProductionOCR("./final_model")
# print(ocr.predict("/kaggle/input/iam-handwriting/test/a01-007-02.png"))



