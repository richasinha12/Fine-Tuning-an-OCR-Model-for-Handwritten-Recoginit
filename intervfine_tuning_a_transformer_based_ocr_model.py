# -*- coding: utf-8 -*-
"""Fine-Tuning a Transformer-Based OCR Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/richasilpi/fine-tuning-a-transformer-based-ocr-model.6b7411e8-0716-4af7-b82a-775111e463a7.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250422/auto/storage/goog4_request%26X-Goog-Date%3D20250422T011626Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da0de58421b969647e60e81f3af0a5bf8da66e1b592e690af3047bd8da191de4e441744f61582339a95f61c158d3885782064819ec7ed2d3a6e2df810bc89da58217c7f2cd769208b213db772fe4bc155fee0a74ccf6ad4506f5af6eb33321fbb91263ae7eeea8551ce052e89e8af1685c1b59645ad5e2bca6a89bbe868c60c9baebc310398d598632638b73f3c8977136aabadf3fd746ea949d53cc2082b494625f73d3660661f2c3686aabd06723d27f8887cceed6fd3dab58416da7c8367238afc4132193d5091133fba056e89bef74b0d5e2d3ff8fa8b11a41376d4a3f2a129bc42e9d1e3fe4f43ee514fe8392ef197186c0c3d37860bc85f9d5a47eb47ae
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
andrianang_imgur5k_sample_path = kagglehub.dataset_download('andrianang/imgur5k-sample')
hongvnphi_iam_handwriting_preprocessed_path = kagglehub.dataset_download('hongvnphi/iam-handwriting-preprocessed')

print('Data source import complete.')

!pip install numpy
!pip install pandas
!pip install Pillow
!pip install torch torchvision
!pip install transformers
!pip install datasets
!pip install evaluate
!pip install jiwer
!pip install wandb

import os
import wandb

# Set wandb API key (replace with your actual key)
os.environ["WANDB_API_KEY"] = "89470e3cb36c534d5103f578c9667a4c0c8131b4"
wandb.login(key=os.environ["WANDB_API_KEY"])

import os
import random
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split
from PIL import Image
import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms
from transformers import (
    VisionEncoderDecoderModel,
    TrOCRProcessor,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    default_data_collator,
    TrainerCallback
)
import evaluate

# Enable cudnn benchmark for potential speed-up (if reproducibility is not the highest priority)
torch.backends.cudnn.benchmark = True

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # For faster training, we are not enforcing strict determinism:
    # torch.backends.cudnn.deterministic = True

set_seed(44)



# -------------------------------
# PART 1: PROCESS IMGUR5K SAMPLE DATASET
# -------------------------------
def load_imgur_labels(labels_file, images_dir, sep="\t"):
    """
    Reads a labels file where each line contains a relative image path and its transcription.
    Returns a DataFrame with columns: image_filename, text, image_path.
    Expected format:
      ./eval_images\image_0.png It
      ./eval_images\image_1.png takes
      ...
    """
    data = []
    with open(labels_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    for line in lines:
        line = line.strip()
        if not line:
            continue
        tokens = line.split()
        if len(tokens) < 2:
            continue
        # Normalize path
        rel_path = tokens[0].replace("\\", "/")
        filename = os.path.basename(rel_path)
        image_path = os.path.join(images_dir, filename)
        text = " ".join(tokens[1:])
        data.append({"image_filename": filename, "text": text, "image_path": image_path})
    return pd.DataFrame(data)

# Base path for IMGUR5K sample dataset on Kaggle
imgur_base_path = "/kaggle/input/imgur5k-sample/"
train_images_dir = os.path.join(imgur_base_path, "train_images")
test_images_dir  = os.path.join(imgur_base_path, "test_images")
eval_images_dir  = os.path.join(imgur_base_path, "eval_images")

train_labels_file = os.path.join(imgur_base_path, "train_labels.txt")
test_labels_file  = os.path.join(imgur_base_path, "test_labels.txt")
eval_labels_file  = os.path.join(imgur_base_path, "eval_labels.txt")

# Load IMGUR5K data
df_imgur_train = load_imgur_labels(train_labels_file, train_images_dir)
df_imgur_test  = load_imgur_labels(test_labels_file, test_images_dir)
df_imgur_eval  = load_imgur_labels(eval_labels_file, eval_images_dir)

print("IMGUR5K splits:")
print("Train:", df_imgur_train.shape)
print("Test:", df_imgur_test.shape)
print("Eval:", df_imgur_eval.shape)

# Combine IMGUR5K train and eval
df_imgur_combined = pd.concat([df_imgur_train, df_imgur_eval], ignore_index=True)
print("Combined IMGUR5K training data shape:", df_imgur_combined.shape)

# -------------------------------
# PART 2: PROCESS IAM HANDWRITING PREPROCESSED DATASET
# -------------------------------
import kagglehub
iam_path = kagglehub.dataset_download("hongvnphi/iam-handwriting-preprocessed")
# Adjust the IAM path if needed:
iam_path = iam_path + '/kaggle/working/iam-handwriting-dataset/'
print("Path to IAM dataset files:", iam_path)

def load_iam_preprocessed(split_folder):
    """
    Reads image and label files from a split folder.
    Expects 'images' and 'labels' subfolders.
    Returns a DataFrame with columns: image_filename, text, image_path.
    """
    images_dir = os.path.join(split_folder, "images")
    labels_dir = os.path.join(split_folder, "labels")
    label_files = [f for f in os.listdir(labels_dir) if f.endswith(".txt")]
    data = []
    for label_file in label_files:
        base = os.path.splitext(label_file)[0]
        image_filename = base + ".png"
        image_path = os.path.join(images_dir, image_filename)
        label_path = os.path.join(labels_dir, label_file)
        with open(label_path, "r", encoding="utf-8") as f:
            label_text = f.read().strip()
        data.append({
            "image_filename": image_filename,
            "text": label_text,
            "image_path": image_path
        })
    return pd.DataFrame(data)

iam_dataset_path = iam_path
df_iam_train = load_iam_preprocessed(os.path.join(iam_dataset_path, "train"))
df_iam_val   = load_iam_preprocessed(os.path.join(iam_dataset_path, "validation"))
df_iam_test  = load_iam_preprocessed(os.path.join(iam_dataset_path, "test"))

print("IAM splits:")
print("Train:", df_iam_train.shape)
print("Validation:", df_iam_val.shape)
print("Test:", df_iam_test.shape)

# -------------------------------
# PART 3: COMBINE ALL DATA SOURCES
# -------------------------------
# For training, combine IAM train and validation with IMGUR5K combined.
df_train_combined = pd.concat([df_iam_train, df_iam_val, df_imgur_combined], ignore_index=True)
# For testing, combine IAM test with IMGUR5K test.
df_test_combined = pd.concat([df_iam_test, df_imgur_test], ignore_index=True)

print("Combined training data shape:", df_train_combined.shape)
print("Combined test data shape:", df_test_combined.shape)

# Shuffle combined training data
df_train_combined = df_train_combined.sample(frac=1, random_state=42).reset_index(drop=True)
# Further split combined training data into final train and validation sets (90% / 10%)
final_train_df, final_val_df = train_test_split(df_train_combined, test_size=0.10, random_state=42)
print("Final train split shape:", final_train_df.shape)
print("Final validation split shape:", final_val_df.shape)

# -------------------------------
# PART 4: SAVE THE SPLITS AS CSV FILES
# -------------------------------
final_train_df.to_csv("combined_train.csv", index=False)
final_val_df.to_csv("combined_val.csv", index=False)
df_test_combined.to_csv("combined_test.csv", index=False)
print("Saved CSV files: combined_train.csv, combined_val.csv, combined_test.csv")

# Import necessary libraries
import os
import pandas as pd
from PIL import Image
import torch
from torchvision import transforms
from torch.utils.data import Dataset
from transformers import (
    TrOCRProcessor,
    VisionEncoderDecoderModel,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    TrainerCallback
)
import evaluate

# ===============================
# CONFIGURATION PARAMETERS
# ===============================
class Config:
    # Fine-tuning hyperparameters
    BATCH_SIZE = 4                # Use 4 for Tesla P100; for dual T4 GPUs, consider increasing this.
    EPOCHS = 4                    # Reduced to 4 epochs for faster turnaround.
    LEARNING_RATE = 5e-5          # Learning rate for the optimizer

    # Paths to CSV files (assumes CSVs already exist with columns "image_path" and "text")
    TRAIN_CSV = "combined_train.csv"
    VAL_CSV = "combined_val.csv"
    TEST_CSV = "combined_test.csv"

    # Base directory for images (if CSV paths are not full paths)
    BASE_IMAGE_DIR = ""

    # Subset sizes to reduce training time
    TRAIN_SIZE = 11000            # Use 11,000 training examples.
    VAL_SIZE = 1500               # Use 1,500 validation examples.

# ===============================
# CUSTOM DATASET DEFINITION
# ===============================
class OCRDataset(Dataset):
    def __init__(self, csv_file, processor, transform=None, max_target_length=128, max_samples=None):
        """
        Initialize the OCRDataset.

        Args:
            csv_file (str): Path to CSV file with columns "image_path" and "text".
            processor: Hugging Face TrOCR processor.
            transform: Image transforms for preprocessing.
            max_target_length (int): Maximum token length for text.
            max_samples (int, optional): Optionally limit the dataset size.
        """
        # Load CSV data into a DataFrame
        self.data = pd.read_csv(csv_file)

        # Limit the dataset size if max_samples is specified
        if max_samples is not None:
            self.data = self.data.head(max_samples)

        # Filter out rows where 'image_path' is missing or empty
        self.data = self.data[self.data["image_path"].notnull() & (self.data["image_path"] != "")]

        self.processor = processor
        self.transform = transform
        self.max_target_length = max_target_length

    def __len__(self):
        # Return the total number of samples in the dataset
        return len(self.data)

    def __getitem__(self, idx):
        # Get a single data sample at the given index
        row = self.data.iloc[idx]

        # Construct full image path if BASE_IMAGE_DIR is provided
        if Config.BASE_IMAGE_DIR:
            image_path = os.path.join(Config.BASE_IMAGE_DIR, row["image_path"])
        else:
            image_path = row["image_path"]

        # Open image and convert to RGB
        image = Image.open(image_path).convert("RGB")

        # Apply transformation if available
        if self.transform:
            image = self.transform(image)

        # Convert text to string (in case it isn't)
        text = str(row["text"])

        return {"image": image, "text": text}

# ===============================
# TRANSFORMATIONS FOR DATA PREPROCESSING
# ===============================

# Define normalization transformation (example values: mean and std set to 0.5)
normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])

# Training transforms: resize to 224x224 and apply augmentations
train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ColorJitter(brightness=0.5, hue=0.3),
    transforms.RandomRotation(degrees=10),
    transforms.ToTensor(),
])

# Validation transforms: only resize and convert to tensor
val_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# ===============================
# LOAD PRE-TRAINED MODEL AND PROCESSOR
# ===============================

# Load the pre-trained TrOCR model and processor
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-large-handwritten")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-large-handwritten")

# Set device to GPU if available, else CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# If multiple GPUs are available, use DataParallel
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs with DataParallel.")
    model = torch.nn.DataParallel(model)

# Ensure necessary token IDs are set for the decoder
if model.config.decoder_start_token_id is None:
    model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
if model.config.pad_token_id is None:
    model.config.pad_token_id = processor.tokenizer.pad_token_id

# ===============================
# CREATE DATASETS FOR TRAINING, VALIDATION, AND TESTING
# ===============================

# Create dataset objects with subset sizes (if defined in Config)
train_dataset = OCRDataset(Config.TRAIN_CSV, processor, transform=train_transforms, max_samples=Config.TRAIN_SIZE)
val_dataset = OCRDataset(Config.VAL_CSV, processor, transform=val_transforms, max_samples=Config.VAL_SIZE)
test_dataset = OCRDataset(Config.TEST_CSV, processor, transform=val_transforms)

# ===============================
# DEFINE DATA COLLATOR FUNCTION
# ===============================
def data_collator(features):
    """
    Collates data into batches, processing images and tokenizing texts.

    Args:
        features (list): List of samples from the dataset.

    Returns:
        dict: Batch containing pixel values and labels.
    """
    # Filter out any samples missing required keys
    valid_features = [f for f in features if "image" in f and "text" in f]
    if len(valid_features) < len(features):
        print("Warning: Filtering out some samples missing required keys.")
    if not valid_features:
        raise ValueError("All samples in the batch are invalid.")

    # Extract images and texts from the valid features
    images = [f["image"] for f in valid_features]
    texts  = [f["text"] for f in valid_features]

    # Process images using the processor (disable rescaling as images are already normalized)
    pixel_values = processor(images=images, return_tensors="pt", do_rescale=False).pixel_values

    # Tokenize texts with specified padding and truncation
    labels = processor.tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    ).input_ids

    # Replace padding token IDs with -100 to ignore them in the loss computation
    labels[labels == processor.tokenizer.pad_token_id] = -100

    return {"pixel_values": pixel_values, "labels": labels}

# ===============================
# SETUP EVALUATION METRICS
# ===============================
# Load evaluation metrics for Character Error Rate (CER) and Word Error Rate (WER)
cer_metric = evaluate.load("cer")
wer_metric = evaluate.load("wer")

def compute_metrics(eval_pred):
    """
    Compute evaluation metrics for the model.

    Args:
        eval_pred (tuple): Tuple containing predictions and labels.

    Returns:
        dict: Dictionary with computed CER and WER.
    """

    predictions, labels = eval_pred

    # Replace -100 in predictions and labels with pad_token_id to avoid decoding errors
    predictions = np.where(predictions == -100, processor.tokenizer.pad_token_id, predictions)
    labels = np.where(labels == -100, processor.tokenizer.pad_token_id, labels)

    # Decode predictions and labels into strings
    pred_str = processor.batch_decode(predictions, skip_special_tokens=True)
    label_str = processor.batch_decode(labels, skip_special_tokens=True)

    # Compute CER and WER using the respective metrics
    cer = cer_metric.compute(predictions=pred_str, references=label_str)
    wer = wer_metric.compute(predictions=pred_str, references=label_str)

    return {"cer": cer, "wer": wer}


# ===============================
# CUSTOM TRAINER CALLBACK FOR LOGGING
# ===============================
class LoggingCallback(TrainerCallback):
    def on_train_begin(self, args, state, control, **kwargs):
        print("Training is starting...")

    def on_epoch_begin(self, args, state, control, **kwargs):
        print(f"Epoch {state.epoch:.2f} is starting...")

    def on_epoch_end(self, args, state, control, **kwargs):
        if state.log_history:
            print(f"Epoch {state.epoch:.2f} finished. Latest metrics: {state.log_history[-1]}")
        else:
            print(f"Epoch {state.epoch:.2f} finished.")

    def on_train_end(self, args, state, control, **kwargs):
        print("Training has finished.")

# ===============================
# SETUP TRAINING ARGUMENTS AND TRAINER
# ===============================
training_args = Seq2SeqTrainingArguments(
    output_dir="./trocr_finetuned",
    per_device_train_batch_size=Config.BATCH_SIZE,
    per_device_eval_batch_size=Config.BATCH_SIZE,
    num_train_epochs=Config.EPOCHS,
    eval_strategy="epoch",              # Evaluation strategy (evaluates at the end of each epoch)
    save_strategy="epoch",              # Save checkpoint at the end of each epoch
    learning_rate=Config.LEARNING_RATE,
    fp16=True,                        # Enable mixed precision training
    logging_steps=50,
    logging_first_step=True,
    log_level="info",
    save_total_limit=2,               # Limit the total number of saved checkpoints
    predict_with_generate=True,
    dataloader_num_workers=4,         # Adjust based on available CPU cores
    report_to=["wandb"],
    logging_dir="./logs",
    run_name="trocr_finetuning_experiment",
    remove_unused_columns=False
)

# Initialize the Seq2SeqTrainer with model, datasets, and other configurations
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Add the custom logging callback to the trainer
trainer.add_callback(LoggingCallback)

# ===============================
# TRAIN THE MODEL
# ===============================
trainer.train()

# Save the fine-tuned model and processor
trainer.save_model("./trocr_finetuned_final")
processor.save_pretrained("./trocr_finetuned_final")


# ===============================
# DEFINE INFERENCE FUNCTION FOR OCR
# ===============================
def ocr_inference(image_path):
    """
    Perform OCR inference on a given image.

    Args:
        image_path (str): Path to the image file.

    Returns:
        str: The predicted text from the image.
    """
    # Open and convert the image to RGB
    image = Image.open(image_path).convert("RGB")

    # Apply validation transformations
    image = val_transforms(image)

    # Process image and move to the same device as the model
    pixel_values = processor(images=image, return_tensors="pt", do_rescale=False).pixel_values.to(device)

    # Generate predictions using the model
    generated_ids = model.generate(pixel_values)

    # Decode the generated IDs to text
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return generated_text

# ===============================
# SAMPLE INFERENCE
# ===============================
# Use the first image from the training CSV as a sample
sample_image_path = pd.read_csv(Config.TRAIN_CSV).iloc[0]["image_path"]
print("OCR Output:", ocr_inference(sample_image_path))

# ===============================
# EVALUATE THE MODEL ON TEST DATASET
# ===============================
test_metrics = trainer.evaluate(test_dataset)
print("Test Metrics:", test_metrics)

# Define target metrics for evaluation
target_cer = 0.07  # 7% target CER
target_wer = 0.15  # 15% target WER

if test_metrics["eval_cer"] <= target_cer and test_metrics["eval_wer"] <= target_wer:
    print("Target metrics achieved: CER <= 7% and WER <= 15%")
else:
    print(f"Target metrics not achieved. CER: {test_metrics['eval_cer']:.4f}, WER: {test_metrics['eval_wer']:.4f}")

# # %% [code]
# # Install required packages (uncomment if running in a new environment)
# !pip install transformers datasets evaluate torch torchvision pillow jiwer

# # ===============================
# # IMPORTS AND SEED SETTING
# # ===============================
# import os
# import random
# import numpy as np
# import pandas as pd
# from PIL import Image
# import torch
# from torch.utils.data import Dataset
# import torchvision.transforms as transforms
# from transformers import (
#     VisionEncoderDecoderModel,
#     TrOCRProcessor,
#     Seq2SeqTrainer,
#     Seq2SeqTrainingArguments,
#     default_data_collator,
#     TrainerCallback
# )
# import evaluate

# def set_seed(seed=42):
#     random.seed(seed)
#     np.random.seed(seed)
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
#     # For faster training, disable deterministic mode:
#     torch.backends.cudnn.deterministic = False
#     torch.backends.cudnn.benchmark = True

# set_seed(42)

# # ===============================
# # CONFIGURATIONS
# # ===============================
# class Config:
#     # Fine-tuning hyperparameters
#     BATCH_SIZE = 4                # Use 4 for Tesla P100; for dual T4s, consider increasing this.
#     EPOCHS = 4                    # Reduced to 4 epochs for faster turnaround.
#     LEARNING_RATE = 5e-5

#     # Paths to CSV files
#     TRAIN_CSV = "combined_train.csv"
#     VAL_CSV = "combined_val.csv"
#     TEST_CSV = "combined_test.csv"

#     # Set BASE_IMAGE_DIR if your image paths are relative
#     BASE_IMAGE_DIR = ""

#     # Subset sizes to reduce training time
#     TRAIN_SIZE = 11000            # Use 11,000 training examples.
#     VAL_SIZE = 1500               # Use 1,500 validation examples.

# # ===============================
# # CUSTOM DATASET DEFINITION
# # ===============================
# class OCRDataset(Dataset):
#     def __init__(self, csv_file, processor, transform=None, max_target_length=128, max_samples=None):
#         """
#         Args:
#             csv_file (str): Path to CSV file with columns "image_path" and "text".
#             processor: Hugging Face TrOCR processor.
#             transform: Optional image transforms.
#             max_target_length (int): Maximum token length for text.
#             max_samples (int, optional): Limits the dataset to this many samples.
#         """
#         self.data = pd.read_csv(csv_file)
#         # Optionally limit the dataset size for faster training.
#         if max_samples is not None:
#             self.data = self.data.head(max_samples)
#         # Ensure image_path values are not null or empty.
#         self.data = self.data[self.data["image_path"].notnull() & (self.data["image_path"] != "")]
#         self.processor = processor
#         self.transform = transform
#         self.max_target_length = max_target_length

#     def __len__(self):
#         return len(self.data)

#     def __getitem__(self, idx):
#         row = self.data.iloc[idx]
#         image_path = os.path.join(Config.BASE_IMAGE_DIR, row["image_path"]) if Config.BASE_IMAGE_DIR else row["image_path"]
#         image = Image.open(image_path).convert("RGB")
#         if self.transform:
#             image = self.transform(image)
#         text = str(row["text"])
#         return {"image": image, "text": text}

# # ===============================
# # IMAGE TRANSFORMS: RESIZE, NORMALIZATION, & AUGMENTATION
# # ===============================
# # Lower resolution to 224x224 to reduce computational load.
# train_transforms = transforms.Compose([
#     transforms.Resize((224, 224)),
#     transforms.ColorJitter(brightness=0.5, hue=0.3),
#     transforms.RandomRotation(degrees=10),
#     transforms.ToTensor(),  # Image values will be in [0, 1]
# ])
# val_transforms = transforms.Compose([
#     transforms.Resize((224, 224)),
#     transforms.ToTensor(),
# ])

# # ===============================
# # LOAD PROCESSOR AND MODEL
# # ===============================
# # Using the large model here; if training speed is critical, consider using the small version.
# processor = TrOCRProcessor.from_pretrained("microsoft/trocr-large-handwritten")
# model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-large-handwritten")
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model.to(device)
# if torch.cuda.device_count() > 1:
#     print(f"Using {torch.cuda.device_count()} GPUs with DataParallel.")
#     model = torch.nn.DataParallel(model)

# # Ensure required tokens are set:
# if model.config.decoder_start_token_id is None:
#     model.config.decoder_start_token_id = processor.tokenizer.bos_token_id  # or use cls_token_id if needed
# if model.config.pad_token_id is None:
#     model.config.pad_token_id = processor.tokenizer.pad_token_id

# # ===============================
# # CREATE DATASETS (using subset sizes)
# # ===============================
# train_dataset = OCRDataset(Config.TRAIN_CSV, processor, transform=train_transforms, max_samples=Config.TRAIN_SIZE)
# val_dataset = OCRDataset(Config.VAL_CSV, processor, transform=val_transforms, max_samples=Config.VAL_SIZE)
# test_dataset = OCRDataset(Config.TEST_CSV, processor, transform=val_transforms)

# # ===============================
# # DEFINE DATA COLLATOR
# # ===============================
# def data_collator(features):
#     valid_features = [f for f in features if "image" in f and "text" in f]
#     if len(valid_features) < len(features):
#         print("Warning: Filtering out some samples missing required keys.")
#     if not valid_features:
#         raise ValueError("All samples in the batch are invalid.")

#     images = [f["image"] for f in valid_features]
#     texts  = [f["text"] for f in valid_features]

#     # Process images to pixel values using processor (disable rescaling as images are already normalized)
#     pixel_values = processor(images=images, return_tensors="pt", do_rescale=False).pixel_values
#     # Tokenize texts
#     labels = processor.tokenizer(
#         texts,
#         padding="max_length",
#         truncation=True,
#         max_length=128,
#         return_tensors="pt"
#     ).input_ids
#     labels[labels == processor.tokenizer.pad_token_id] = -100
#     return {"pixel_values": pixel_values, "labels": labels}

# # ===============================
# # SETUP METRICS (CER and WER)
# # ===============================
# cer_metric = evaluate.load("cer")
# wer_metric = evaluate.load("wer")
# def compute_metrics(eval_pred):
#     predictions, labels = eval_pred
#     pred_str = processor.batch_decode(predictions, skip_special_tokens=True)
#     labels[labels == -100] = processor.tokenizer.pad_token_id
#     label_str = processor.batch_decode(labels, skip_special_tokens=True)
#     cer = cer_metric.compute(predictions=pred_str, references=label_str)
#     wer = wer_metric.compute(predictions=pred_str, references=label_str)
#     return {"cer": cer, "wer": wer}

# # ===============================
# # DEFINE A CUSTOM LOGGING CALLBACK
# # ===============================
# class LoggingCallback(TrainerCallback):
#     def on_train_begin(self, args, state, control, **kwargs):
#         print("Training is starting...")
#     def on_epoch_begin(self, args, state, control, **kwargs):
#         print(f"Epoch {state.epoch:.2f} is starting...")
#     def on_epoch_end(self, args, state, control, **kwargs):
#         print(f"Epoch {state.epoch:.2f} has finished. Metrics: {state.log_history[-1] if state.log_history else 'N/A'}")
#     def on_train_end(self, args, state, control, **kwargs):
#         print("Training has finished.")

# # ===============================
# # TRAINING ARGUMENTS
# # ===============================
# training_args = Seq2SeqTrainingArguments(
#     output_dir="./trocr_finetuned",
#     per_device_train_batch_size=Config.BATCH_SIZE,
#     per_device_eval_batch_size=Config.BATCH_SIZE,
#     num_train_epochs=Config.EPOCHS,
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=Config.LEARNING_RATE,
#     fp16=True,                        # Enable mixed precision training
#     logging_steps=50,
#     save_total_limit=2,
#     predict_with_generate=True,
#     dataloader_num_workers=8,         # Increase workers for faster data loading (if CPU permits)
#     report_to=["wandb"],
#     logging_dir="./logs",
#     remove_unused_columns=False
# )

# # ===============================
# # INITIALIZE THE TRAINER WITH CALLBACKS
# # ===============================
# trainer = Seq2SeqTrainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     data_collator=data_collator,
#     compute_metrics=compute_metrics,
# )
# trainer.add_callback(LoggingCallback)

# # ===============================
# # TRAIN THE MODEL
# # ===============================
# trainer.train()

# # ===============================
# # SAVE THE FINE-TUNED MODEL
# # ===============================
# trainer.save_model("./trocr_finetuned_final")
# processor.save_pretrained("./trocr_finetuned_final")

# # ===============================
# # SIMPLE INFERENCE FUNCTION
# # ===============================
# def ocr_inference(image_path):
#     image = Image.open(image_path).convert("RGB")
#     image = val_transforms(image)
#     pixel_values = processor(images=image, return_tensors="pt", do_rescale=False).pixel_values.to(device)
#     generated_ids = model.generate(pixel_values)
#     generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
#     return generated_text

# # Test inference on a sample image from the training dataset (or any dataset)
# sample_image_dir = os.path.dirname(pd.read_csv(Config.TRAIN_CSV).iloc[0]["image_path"])
# sample_image = os.listdir(sample_image_dir)[0]
# sample_image_path = os.path.join(sample_image_dir, sample_image)
# print("OCR Output:", ocr_inference(sample_image_path))

