{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies\n!pip install -q datasets transformers[torch] evaluate jiwer torchvision opencv-python-headless","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:50:29.392782Z","iopub.execute_input":"2025-04-15T03:50:29.393342Z","iopub.status.idle":"2025-04-15T03:51:47.304449Z","shell.execute_reply.started":"2025-04-15T03:50:29.393318Z","shell.execute_reply":"2025-04-15T03:51:47.303774Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:52:27.682370Z","iopub.execute_input":"2025-04-15T03:52:27.683174Z","iopub.status.idle":"2025-04-15T03:52:39.015568Z","shell.execute_reply.started":"2025-04-15T03:52:27.683145Z","shell.execute_reply":"2025-04-15T03:52:39.014868Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\nSuccessfully installed transformers-4.51.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#%%script echo skipping\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:52:43.017777Z","iopub.execute_input":"2025-04-15T03:52:43.018042Z","iopub.status.idle":"2025-04-15T03:52:43.021661Z","shell.execute_reply.started":"2025-04-15T03:52:43.018024Z","shell.execute_reply":"2025-04-15T03:52:43.021056Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom datasets import load_dataset, DatasetDict, concatenate_datasets\nfrom transformers import (\n    TrOCRProcessor,\n    VisionEncoderDecoderModel,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    default_data_collator,\n    EarlyStoppingCallback\n)\nimport editdistance  # Required for CER computation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:52:46.475936Z","iopub.execute_input":"2025-04-15T03:52:46.476503Z","iopub.status.idle":"2025-04-15T03:53:12.870330Z","shell.execute_reply.started":"2025-04-15T03:52:46.476479Z","shell.execute_reply":"2025-04-15T03:53:12.869763Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 03:52:57.180360: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744689177.393324      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744689177.461378      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Kaggle-specific configuration\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_GPUS = torch.cuda.device_count()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:53:29.506585Z","iopub.execute_input":"2025-04-15T03:53:29.507183Z","iopub.status.idle":"2025-04-15T03:53:29.511377Z","shell.execute_reply.started":"2025-04-15T03:53:29.507158Z","shell.execute_reply":"2025-04-15T03:53:29.510441Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Training parameters\nMODEL_NAME = \"microsoft/trocr-base-handwritten\"\nMAX_EPOCHS = 10\nLEARNING_RATE = 5e-5\nIMAGE_SIZE = (384, 384)\nBATCH_SIZE = 8 if \"T4\" in torch.cuda.get_device_name(0) else 4\nBATCH_SIZE *= NUM_GPUS\nGRADIENT_ACCUMULATION_STEPS = 2 if NUM_GPUS == 1 else 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:53:31.851998Z","iopub.execute_input":"2025-04-15T03:53:31.852620Z","iopub.status.idle":"2025-04-15T03:53:31.856514Z","shell.execute_reply.started":"2025-04-15T03:53:31.852590Z","shell.execute_reply":"2025-04-15T03:53:31.855907Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Training parameters\nMODEL_NAME = \"microsoft/trocr-base-handwritten\"\nMAX_EPOCHS = 4\nLEARNING_RATE = 5e-5\nIMAGE_SIZE = (384, 384)\nBATCH_SIZE = 8 if \"T4\" in torch.cuda.get_device_name(0) else 4\nBATCH_SIZE *= NUM_GPUS\nGRADIENT_ACCUMULATION_STEPS = 2 if NUM_GPUS == 1 else 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\niam_dataset = load_dataset(\"Teklia/IAM-line\")\nimgur_dataset = load_dataset(\"staghado/IMGUR-dataset\")\n\n# Combine training datasets\ncombined_train_dataset = concatenate_datasets([iam_dataset[\"train\"], imgur_dataset[\"train\"]])\ntrain_val_split = combined_train_dataset.train_test_split(test_size=0.1, seed=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:53:35.444106Z","iopub.execute_input":"2025-04-15T03:53:35.444764Z","iopub.status.idle":"2025-04-15T03:57:57.503184Z","shell.execute_reply.started":"2025-04-15T03:53:35.444743Z","shell.execute_reply":"2025-04-15T03:57:57.502622Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45921f89fe3d4c6ab9a20f23d5314a01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.parquet:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4d80c8c12b74fb88d185257f48de9e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.parquet:   0%|          | 0.00/24.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6390924c842c49bd8a5ccfd3dfa164af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.parquet:   0%|          | 0.00/73.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26fb0592e19e440f8d13d9f5f44363aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/6482 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fbc45be0c0d4b0b95e1ab71beef1b8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/976 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fcc414738a44653a4626383381e8f2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2915 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d3eca070e94154993f86f485667851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IMGUR5K-Handwriting-Dataset.zip:   0%|          | 0.00/4.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b58effca1fc54f9c8d75442198b0edc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_0.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c82d6a8e1764f3abb0700ed8fe3aff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_1.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a380f27c21a94a18a18e1d52eab909bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_2.zip:   0%|          | 0.00/2.49G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"711d4f44779b4db081f967638839978e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_3.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e188d3ebae446c081a4c8ba380d0bef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_4.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da5d747ec8ab40d4ba7fc6a23c4ba737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_5.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b2ded2b1c74ff1b827455174b933c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_6.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5fe669aef77458783efeeabfe3812bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_7.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a2151cf6ab34cadb953d655388728ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_8.zip:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd9aa4c22314488fbbfc8cc4da194cd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chunk_9.zip:   0%|          | 0.00/2.36G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db8ebaf61c54197befeba8787b95a33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_info.zip:   0%|          | 0.00/15.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac1ebc5755e4c96be7561fcd01d1acd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"imgur8k-dataset.zip:   0%|          | 0.00/4.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ea5613f7bc049e28124393ec14564cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"398beeeffad144fe957a5de8fd8bae21"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Define OCR Preprocessor\nclass OCRPreprocessor:\n    def __init__(self, model_name, image_size=(224, 224)):\n        self.processor = TrOCRProcessor.from_pretrained(model_name)\n        self.image_size = image_size\n\n    def _process_image(self, img):\n        if img.mode != \"L\":\n            img = img.convert(\"L\")\n        img = np.array(img)\n        img = cv2.fastNlMeansDenoising(img, h=10)\n        img = cv2.medianBlur(img, 3)\n        img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n        img = cv2.resize(img, self.image_size)\n        return img\n\n    def process_batch(self, examples):\n        processed_images = []\n        for img_data in examples[\"image\"]:\n            img = Image.open(img_data) if isinstance(img_data, str) else img_data\n            processed_img = Image.fromarray(self._process_image(img)).convert(\"RGB\")\n            processed_images.append(processed_img)\n\n        texts = [str(text) if text is not None else \"\" for text in examples[\"text\"]]\n        pixel_values = self.processor(processed_images, return_tensors=\"pt\").pixel_values\n        labels = self.processor.tokenizer(\n            texts, padding=\"max_length\", max_length=64, return_tensors=\"pt\"\n        ).input_ids\n\n        return {\"pixel_values\": pixel_values, \"labels\": labels}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:58:14.783916Z","iopub.execute_input":"2025-04-15T03:58:14.784191Z","iopub.status.idle":"2025-04-15T03:58:14.790957Z","shell.execute_reply.started":"2025-04-15T03:58:14.784173Z","shell.execute_reply":"2025-04-15T03:58:14.790153Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Preprocess datasets with a subset (limit to 400 examples)\npreprocessor = OCRPreprocessor(model_name=MODEL_NAME, image_size=IMAGE_SIZE)\n\n# Take only the first 400 examples for training\nlimited_train_dataset = train_val_split[\"train\"].select(range(400))\n\n# Preprocess the limited dataset\ntrain_dataset = limited_train_dataset.map(\n    preprocessor.process_batch,\n    batched=True,\n    batch_size=16,\n    remove_columns=[\"image\", \"text\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:58:19.702416Z","iopub.execute_input":"2025-04-15T03:58:19.702689Z","iopub.status.idle":"2025-04-15T04:04:39.917568Z","shell.execute_reply.started":"2025-04-15T03:58:19.702667Z","shell.execute_reply":"2025-04-15T04:04:39.916826Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a8ca79e1324770b6ebae5ef11694cc"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"794bae6987df463a992b88bcfacd264e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ed6d0ada5a4a98a03741b32ebec7ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0245e0fd310c4b1f860a139ca851a8cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030fa9ec465a4dc1bb121cee72f301d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbb2f2c23391460689acb9faf6ee2b0a"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"print(train_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:04:56.783240Z","iopub.execute_input":"2025-04-15T04:04:56.783913Z","iopub.status.idle":"2025-04-15T04:04:56.787936Z","shell.execute_reply.started":"2025-04-15T04:04:56.783891Z","shell.execute_reply":"2025-04-15T04:04:56.787152Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['pixel_values', 'labels'],\n    num_rows: 400\n})\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Take only the first 100 examples for validation\nvalidation_subset = train_val_split[\"test\"].select(range(10))\n\n# Preprocess the validation dataset\nvalidation_dataset = validation_subset.map(\n    preprocessor.process_batch,\n    batched=True,\n    batch_size=16,\n    remove_columns=[\"image\", \"text\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:04:58.997988Z","iopub.execute_input":"2025-04-15T04:04:58.998798Z","iopub.status.idle":"2025-04-15T04:05:08.463255Z","shell.execute_reply.started":"2025-04-15T04:04:58.998764Z","shell.execute_reply":"2025-04-15T04:05:08.462492Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33319ac4398048f583f39e920ea195d0"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Load and configure the model\nmodel = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\nmodel.config.decoder_start_token_id = preprocessor.processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = preprocessor.processor.tokenizer.pad_token_id\nmodel.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:05:19.784576Z","iopub.execute_input":"2025-04-15T04:05:19.785170Z","iopub.status.idle":"2025-04-15T04:05:30.378963Z","shell.execute_reply.started":"2025-04-15T04:05:19.785141Z","shell.execute_reply":"2025-04-15T04:05:30.378313Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f97f4438a649a68608adacdd0ade86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73fa7072602410795b31c6b161fb105"}},"metadata":{}},{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"941d9d0fa24146288761a76347fedb69"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Define CER computation\ndef calculate_cer(references, predictions):\n    total_chars = sum(len(ref) for ref in references)\n    total_errors = sum(editdistance.eval(ref, pred) for ref, pred in zip(references, predictions))\n    return total_errors / total_chars if total_chars > 0 else 1.0\n\n# Define WER computation\ndef calculate_wer(references, predictions):\n    total_words = sum(len(ref.split()) for ref in references)\n    total_errors = sum(editdistance.eval(ref.split(), pred.split()) for ref, pred in zip(references, predictions))\n    return total_errors / total_words if total_words > 0 else 1.0\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:05:46.858731Z","iopub.execute_input":"2025-04-15T04:05:46.859064Z","iopub.status.idle":"2025-04-15T04:05:46.866283Z","shell.execute_reply.started":"2025-04-15T04:05:46.859037Z","shell.execute_reply":"2025-04-15T04:05:46.865531Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Corrected training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=MAX_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE // NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE // NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    predict_with_generate=True,\n    generation_max_length=64,\n    metric_for_best_model=\"cer\",\n    load_best_model_at_end=True,\n    greater_is_better=False,\n    report_to=\"none\",\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:15:25.332952Z","iopub.execute_input":"2025-04-15T04:15:25.333241Z","iopub.status.idle":"2025-04-15T04:15:25.368503Z","shell.execute_reply.started":"2025-04-15T04:15:25.333222Z","shell.execute_reply":"2025-04-15T04:15:25.367756Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from transformers import TrOCRProcessor, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n\n# Load the processor\nprocessor = TrOCRProcessor.from_pretrained(MODEL_NAME)\n\n# Define the compute_metrics function\ndef compute_metrics(pred):\n    predictions = pred.predictions\n    labels = pred.label_ids\n\n    # Decode predictions and labels\n    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Filter valid pairs (ensure no empty labels)\n    valid_pairs = [\n        (pred.strip(), label.strip())\n        for pred, label in zip(decoded_preds, decoded_labels)\n        if label.strip()\n    ]\n    if not valid_pairs:  # If all references are empty\n        return {\"cer\": 1.0, \"wer\": 1.0}\n\n    decoded_preds, decoded_labels = zip(*valid_pairs)\n\n    # Compute metrics\n    cer = calculate_cer(decoded_labels, decoded_preds)  # Implement this\n    wer = calculate_wer(decoded_labels, decoded_preds)  # Implement this\n    return {\"cer\": cer, \"wer\": wer}\n\n# Initialize trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=default_data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:20:43.840715Z","iopub.execute_input":"2025-04-15T04:20:43.841444Z","iopub.status.idle":"2025-04-15T04:30:46.358503Z","shell.execute_reply.started":"2025-04-15T04:20:43.841418Z","shell.execute_reply":"2025-04-15T04:30:46.357940Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/250 09:51 < 15:05, 0.17 it/s, Epoch 4/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Cer</th>\n      <th>Wer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.000002</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.000026</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.048700</td>\n      <td>0.000002</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['decoder.output_projection.weight'].\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=0.04868200778961182, metrics={'train_runtime': 600.9212, 'train_samples_per_second': 6.656, 'train_steps_per_second': 0.416, 'total_flos': 1.1972563055935488e+18, 'train_loss': 0.04868200778961182, 'epoch': 4.0})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Subset IAM test dataset for evaluation (limit to 100 examples)\nsubset_test_dataset = iam_dataset[\"test\"].select(range(300))  # Take only the first 100 examples\n\n# Preprocess the test dataset\nprocessed_test_dataset = subset_test_dataset.map(\n    preprocessor.process_batch,\n    batched=True,\n    batch_size=16,\n    remove_columns=[\"image\", \"text\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:42:41.862789Z","iopub.execute_input":"2025-04-15T04:42:41.863498Z","iopub.status.idle":"2025-04-15T04:43:37.236208Z","shell.execute_reply.started":"2025-04-15T04:42:41.863476Z","shell.execute_reply":"2025-04-15T04:43:37.235447Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03318d09368c4bf5af46c075ac536bc4"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# Evaluate the model on the IAM test subset\nresults = trainer.evaluate(processed_test_dataset, metric_key_prefix=\"test\")\nprint(f\"Test CER (IAM dataset subset): {results['test_cer']*100:.2f}%\")\nprint(f\"Test WER (IAM dataset subset): {results['test_wer']*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T04:44:20.213515Z","iopub.execute_input":"2025-04-15T04:44:20.213786Z","iopub.status.idle":"2025-04-15T04:45:59.667966Z","shell.execute_reply.started":"2025-04-15T04:44:20.213763Z","shell.execute_reply":"2025-04-15T04:45:59.667266Z"}},"outputs":[{"name":"stderr","text":"early stopping required metric_for_best_model, but did not find eval_cer so early stopping is disabled\n","output_type":"stream"},{"name":"stdout","text":"Test CER (IAM dataset subset): 100.00%\nTest WER (IAM dataset subset): 100.00%\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Check if the target CER and WER are met\nif results['test_cer'] * 100 <= 7 and results['test_wer'] * 100 <= 15:\n    print(\"Target achieved: CER ≤ 7% and WER ≤ 15%\")\nelse:\n    print(\"Target not achieved. Adjust training or preprocessing.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom datasets import load_dataset, DatasetDict, concatenate_datasets\nfrom transformers import (\n    TrOCRProcessor,\n    VisionEncoderDecoderModel,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    default_data_collator,\n    EarlyStoppingCallback\n)\nimport editdistance  # Required for CER computation\n\n# Kaggle-specific configuration\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_GPUS = torch.cuda.device_count()\n\n# Training parameters\nMODEL_NAME = \"microsoft/trocr-base-handwritten\"\nMAX_EPOCHS = 10\nLEARNING_RATE = 5e-5\nIMAGE_SIZE = (384, 384)\nBATCH_SIZE = 8 if \"T4\" in torch.cuda.get_device_name(0) else 4\nBATCH_SIZE *= NUM_GPUS\nGRADIENT_ACCUMULATION_STEPS = 2 if NUM_GPUS == 1 else 1\n\n# Define CER computation\ndef calculate_cer(references, predictions):\n    total_chars = sum(len(ref) for ref in references)\n    total_errors = sum(editdistance.eval(ref, pred) for ref, pred in zip(references, predictions))\n    return total_errors / total_chars if total_chars > 0 else 1.0\n\n# Define WER computation\ndef calculate_wer(references, predictions):\n    total_words = sum(len(ref.split()) for ref in references)\n    total_errors = sum(editdistance.eval(ref.split(), pred.split()) for ref, pred in zip(references, predictions))\n    return total_errors / total_words if total_words > 0 else 1.0\n\n# Define OCR Preprocessor\nclass OCRPreprocessor:\n    def __init__(self, model_name, image_size=(224, 224)):\n        self.processor = TrOCRProcessor.from_pretrained(model_name)\n        self.image_size = image_size\n\n    def _process_image(self, img):\n        if img.mode != \"L\":\n            img = img.convert(\"L\")\n        img = np.array(img)\n        img = cv2.fastNlMeansDenoising(img, h=10)\n        img = cv2.medianBlur(img, 3)\n        img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n        img = cv2.resize(img, self.image_size)\n        return img\n\n    def process_batch(self, examples):\n        processed_images = []\n        for img_data in examples[\"image\"]:\n            img = Image.open(img_data) if isinstance(img_data, str) else img_data\n            processed_img = Image.fromarray(self._process_image(img)).convert(\"RGB\")\n            processed_images.append(processed_img)\n\n        texts = [str(text) if text is not None else \"\" for text in examples[\"text\"]]\n        pixel_values = self.processor(processed_images, return_tensors=\"pt\").pixel_values\n        labels = self.processor.tokenizer(\n            texts, padding=\"max_length\", max_length=64, return_tensors=\"pt\"\n        ).input_ids\n\n        return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n# Load datasets\niam_dataset = load_dataset(\"Teklia/IAM-line\")\nimgur_dataset = load_dataset(\"staghado/IMGUR-dataset\")\n\n# Combine training datasets\ncombined_train_dataset = concatenate_datasets([iam_dataset[\"train\"], imgur_dataset[\"train\"]])\ntrain_val_split = combined_train_dataset.train_test_split(test_size=0.1, seed=42)\n\n# Preprocess datasets\npreprocessor = OCRPreprocessor(model_name=MODEL_NAME, image_size=IMAGE_SIZE)\ntrain_dataset = train_val_split[\"train\"].map(\n    preprocessor.process_batch,\n    batched=True,\n    batch_size=16,\n    remove_columns=[\"image\", \"text\"]\n)\nvalidation_dataset = train_val_split[\"test\"].map(\n    preprocessor.process_batch,\n    batched=True,\n    batch_size=16,\n    remove_columns=[\"image\", \"text\"]\n)\n\n# Subset IAM test dataset for evaluation\nsubset_test_dataset = iam_dataset[\"test\"].select(range(500))\nprocessed_test_dataset = subset_test_dataset.map(\n    preprocessor.process_batch,\n    batched=True,\n    batch_size=16,\n    remove_columns=[\"image\", \"text\"]\n)\n\n# Load and configure the model\nmodel = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\nmodel.config.decoder_start_token_id = preprocessor.processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = preprocessor.processor.tokenizer.pad_token_id\nmodel.to(DEVICE)\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=MAX_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE // NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE // NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    predict_with_generate=True,\n    generation_max_length=64,\n    metric_for_best_model=\"cer\",\n    load_best_model_at_end=True,\n    greater_is_better=False,\n    report_to=\"none\",\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n)\n\n# Define compute metrics\ndef compute_metrics(pred):\n    predictions = pred.predictions\n    labels = pred.label_ids\n\n    decoded_preds = preprocessor.processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = preprocessor.processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = zip(\n        *[(pred.strip(), label.strip()) for pred, label in zip(decoded_preds, decoded_labels) if label.strip()]\n    ) if decoded_labels else ([], [])\n\n    cer = calculate_cer(decoded_labels, decoded_preds)\n    wer = calculate_wer(decoded_labels, decoded_preds)\n\n    return {\"cer\": cer, \"wer\": wer}\n\n# Initialize trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=default_data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model on the IAM test subset\nresults = trainer.evaluate(processed_test_dataset, metric_key_prefix=\"test\")\nprint(f\"Test CER (IAM dataset subset): {results['test_cer']*100:.2f}%\")\nprint(f\"Test WER (IAM dataset subset): {results['test_wer']*100:.2f}%\")\n\n# Check if the target CER and WER are met\nif results['test_cer'] * 100 <= 7 and results['test_wer'] * 100 <= 15:\n    print(\"Target achieved: CER ≤ 7% and WER ≤ 15%\")\nelse:\n    print(\"Target not achieved. Adjust training or preprocessing.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}