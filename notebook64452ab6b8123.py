# -*- coding: utf-8 -*-
"""notebook64452ab6b8123.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n7wXwCYBT1nZxY0wD0c1W0jXTSElMF-6
"""

# Install dependencies
!pip install -q datasets transformers[torch] evaluate jiwer torchvision opencv-python-headless

!pip install --upgrade transformers

#%%script echo skipping
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import os
import cv2
import torch
import numpy as np
from PIL import Image
from datasets import load_dataset, DatasetDict, concatenate_datasets
from transformers import (
    TrOCRProcessor,
    VisionEncoderDecoderModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    default_data_collator,
    EarlyStoppingCallback
)
import editdistance  # Required for CER computation

# Kaggle-specific configuration
os.environ["TOKENIZERS_PARALLELISM"] = "false"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_GPUS = torch.cuda.device_count()

# Training parameters
MODEL_NAME = "microsoft/trocr-base-handwritten"
MAX_EPOCHS = 10
LEARNING_RATE = 5e-5
IMAGE_SIZE = (384, 384)
BATCH_SIZE = 8 if "T4" in torch.cuda.get_device_name(0) else 4
BATCH_SIZE *= NUM_GPUS
GRADIENT_ACCUMULATION_STEPS = 2 if NUM_GPUS == 1 else 1

# Training parameters
MODEL_NAME = "microsoft/trocr-base-handwritten"
MAX_EPOCHS = 4
LEARNING_RATE = 5e-5
IMAGE_SIZE = (384, 384)
BATCH_SIZE = 8 if "T4" in torch.cuda.get_device_name(0) else 4
BATCH_SIZE *= NUM_GPUS
GRADIENT_ACCUMULATION_STEPS = 2 if NUM_GPUS == 1 else 1

# Load datasets
iam_dataset = load_dataset("Teklia/IAM-line")
imgur_dataset = load_dataset("staghado/IMGUR-dataset")

# Combine training datasets
combined_train_dataset = concatenate_datasets([iam_dataset["train"], imgur_dataset["train"]])
train_val_split = combined_train_dataset.train_test_split(test_size=0.1, seed=42)

# Define OCR Preprocessor
class OCRPreprocessor:
    def __init__(self, model_name, image_size=(224, 224)):
        self.processor = TrOCRProcessor.from_pretrained(model_name)
        self.image_size = image_size

    def _process_image(self, img):
        if img.mode != "L":
            img = img.convert("L")
        img = np.array(img)
        img = cv2.fastNlMeansDenoising(img, h=10)
        img = cv2.medianBlur(img, 3)
        img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
        img = cv2.resize(img, self.image_size)
        return img

    def process_batch(self, examples):
        processed_images = []
        for img_data in examples["image"]:
            img = Image.open(img_data) if isinstance(img_data, str) else img_data
            processed_img = Image.fromarray(self._process_image(img)).convert("RGB")
            processed_images.append(processed_img)

        texts = [str(text) if text is not None else "" for text in examples["text"]]
        pixel_values = self.processor(processed_images, return_tensors="pt").pixel_values
        labels = self.processor.tokenizer(
            texts, padding="max_length", max_length=64, return_tensors="pt"
        ).input_ids

        return {"pixel_values": pixel_values, "labels": labels}

# Preprocess datasets with a subset (limit to 400 examples)
preprocessor = OCRPreprocessor(model_name=MODEL_NAME, image_size=IMAGE_SIZE)

# Take only the first 400 examples for training
limited_train_dataset = train_val_split["train"].select(range(400))

# Preprocess the limited dataset
train_dataset = limited_train_dataset.map(
    preprocessor.process_batch,
    batched=True,
    batch_size=16,
    remove_columns=["image", "text"]
)

print(train_dataset)

# Take only the first 100 examples for validation
validation_subset = train_val_split["test"].select(range(10))

# Preprocess the validation dataset
validation_dataset = validation_subset.map(
    preprocessor.process_batch,
    batched=True,
    batch_size=16,
    remove_columns=["image", "text"]
)

# Load and configure the model
model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)
model.config.decoder_start_token_id = preprocessor.processor.tokenizer.cls_token_id
model.config.pad_token_id = preprocessor.processor.tokenizer.pad_token_id
model.to(DEVICE)

# Define CER computation
def calculate_cer(references, predictions):
    total_chars = sum(len(ref) for ref in references)
    total_errors = sum(editdistance.eval(ref, pred) for ref, pred in zip(references, predictions))
    return total_errors / total_chars if total_chars > 0 else 1.0

# Define WER computation
def calculate_wer(references, predictions):
    total_words = sum(len(ref.split()) for ref in references)
    total_errors = sum(editdistance.eval(ref.split(), pred.split()) for ref, pred in zip(references, predictions))
    return total_errors / total_words if total_words > 0 else 1.0

# Corrected training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    num_train_epochs=MAX_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE // NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE // NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    eval_strategy="epoch",  # Changed from evaluation_strategy
    save_strategy="epoch",
    logging_steps=100,
    predict_with_generate=True,
    generation_max_length=64,
    metric_for_best_model="cer",
    load_best_model_at_end=True,
    greater_is_better=False,
    report_to="none",
    warmup_ratio=0.1,
    weight_decay=0.01,
)

from transformers import TrOCRProcessor, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback

# Load the processor
processor = TrOCRProcessor.from_pretrained(MODEL_NAME)

# Define the compute_metrics function
def compute_metrics(pred):
    predictions = pred.predictions
    labels = pred.label_ids

    # Decode predictions and labels
    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Filter valid pairs (ensure no empty labels)
    valid_pairs = [
        (pred.strip(), label.strip())
        for pred, label in zip(decoded_preds, decoded_labels)
        if label.strip()
    ]
    if not valid_pairs:  # If all references are empty
        return {"cer": 1.0, "wer": 1.0}

    decoded_preds, decoded_labels = zip(*valid_pairs)

    # Compute metrics
    cer = calculate_cer(decoded_labels, decoded_preds)  # Implement this
    wer = calculate_wer(decoded_labels, decoded_preds)  # Implement this
    return {"cer": cer, "wer": wer}

# Initialize trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    compute_metrics=compute_metrics,
    data_collator=default_data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)

# Train the model
trainer.train()

# Subset IAM test dataset for evaluation (limit to 100 examples)
subset_test_dataset = iam_dataset["test"].select(range(300))  # Take only the first 100 examples

# Preprocess the test dataset
processed_test_dataset = subset_test_dataset.map(
    preprocessor.process_batch,
    batched=True,
    batch_size=16,
    remove_columns=["image", "text"]
)

# Evaluate the model on the IAM test subset
results = trainer.evaluate(processed_test_dataset, metric_key_prefix="test")
print(f"Test CER (IAM dataset subset): {results['test_cer']*100:.2f}%")
print(f"Test WER (IAM dataset subset): {results['test_wer']*100:.2f}%")

# Check if the target CER and WER are met
if results['test_cer'] * 100 <= 7 and results['test_wer'] * 100 <= 15:
    print("Target achieved: CER ≤ 7% and WER ≤ 15%")
else:
    print("Target not achieved. Adjust training or preprocessing.")

import os
import cv2
import torch
import numpy as np
from PIL import Image
from datasets import load_dataset, DatasetDict, concatenate_datasets
from transformers import (
    TrOCRProcessor,
    VisionEncoderDecoderModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    default_data_collator,
    EarlyStoppingCallback
)
import editdistance  # Required for CER computation

# Kaggle-specific configuration
os.environ["TOKENIZERS_PARALLELISM"] = "false"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_GPUS = torch.cuda.device_count()

# Training parameters
MODEL_NAME = "microsoft/trocr-base-handwritten"
MAX_EPOCHS = 10
LEARNING_RATE = 5e-5
IMAGE_SIZE = (384, 384)
BATCH_SIZE = 8 if "T4" in torch.cuda.get_device_name(0) else 4
BATCH_SIZE *= NUM_GPUS
GRADIENT_ACCUMULATION_STEPS = 2 if NUM_GPUS == 1 else 1

# Define CER computation
def calculate_cer(references, predictions):
    total_chars = sum(len(ref) for ref in references)
    total_errors = sum(editdistance.eval(ref, pred) for ref, pred in zip(references, predictions))
    return total_errors / total_chars if total_chars > 0 else 1.0

# Define WER computation
def calculate_wer(references, predictions):
    total_words = sum(len(ref.split()) for ref in references)
    total_errors = sum(editdistance.eval(ref.split(), pred.split()) for ref, pred in zip(references, predictions))
    return total_errors / total_words if total_words > 0 else 1.0

# Define OCR Preprocessor
class OCRPreprocessor:
    def __init__(self, model_name, image_size=(224, 224)):
        self.processor = TrOCRProcessor.from_pretrained(model_name)
        self.image_size = image_size

    def _process_image(self, img):
        if img.mode != "L":
            img = img.convert("L")
        img = np.array(img)
        img = cv2.fastNlMeansDenoising(img, h=10)
        img = cv2.medianBlur(img, 3)
        img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
        img = cv2.resize(img, self.image_size)
        return img

    def process_batch(self, examples):
        processed_images = []
        for img_data in examples["image"]:
            img = Image.open(img_data) if isinstance(img_data, str) else img_data
            processed_img = Image.fromarray(self._process_image(img)).convert("RGB")
            processed_images.append(processed_img)

        texts = [str(text) if text is not None else "" for text in examples["text"]]
        pixel_values = self.processor(processed_images, return_tensors="pt").pixel_values
        labels = self.processor.tokenizer(
            texts, padding="max_length", max_length=64, return_tensors="pt"
        ).input_ids

        return {"pixel_values": pixel_values, "labels": labels}

# Load datasets
iam_dataset = load_dataset("Teklia/IAM-line")
imgur_dataset = load_dataset("staghado/IMGUR-dataset")

# Combine training datasets
combined_train_dataset = concatenate_datasets([iam_dataset["train"], imgur_dataset["train"]])
train_val_split = combined_train_dataset.train_test_split(test_size=0.1, seed=42)

# Preprocess datasets
preprocessor = OCRPreprocessor(model_name=MODEL_NAME, image_size=IMAGE_SIZE)
train_dataset = train_val_split["train"].map(
    preprocessor.process_batch,
    batched=True,
    batch_size=16,
    remove_columns=["image", "text"]
)
validation_dataset = train_val_split["test"].map(
    preprocessor.process_batch,
    batched=True,
    batch_size=16,
    remove_columns=["image", "text"]
)

# Subset IAM test dataset for evaluation
subset_test_dataset = iam_dataset["test"].select(range(500))
processed_test_dataset = subset_test_dataset.map(
    preprocessor.process_batch,
    batched=True,
    batch_size=16,
    remove_columns=["image", "text"]
)

# Load and configure the model
model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)
model.config.decoder_start_token_id = preprocessor.processor.tokenizer.cls_token_id
model.config.pad_token_id = preprocessor.processor.tokenizer.pad_token_id
model.to(DEVICE)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    num_train_epochs=MAX_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE // NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE // NUM_GPUS if NUM_GPUS > 1 else BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_steps=100,
    predict_with_generate=True,
    generation_max_length=64,
    metric_for_best_model="cer",
    load_best_model_at_end=True,
    greater_is_better=False,
    report_to="none",
    warmup_ratio=0.1,
    weight_decay=0.01,
)

# Define compute metrics
def compute_metrics(pred):
    predictions = pred.predictions
    labels = pred.label_ids

    decoded_preds = preprocessor.processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = preprocessor.processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds, decoded_labels = zip(
        *[(pred.strip(), label.strip()) for pred, label in zip(decoded_preds, decoded_labels) if label.strip()]
    ) if decoded_labels else ([], [])

    cer = calculate_cer(decoded_labels, decoded_preds)
    wer = calculate_wer(decoded_labels, decoded_preds)

    return {"cer": cer, "wer": wer}

# Initialize trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    compute_metrics=compute_metrics,
    data_collator=default_data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)

# Train the model
trainer.train()

# Evaluate the model on the IAM test subset
results = trainer.evaluate(processed_test_dataset, metric_key_prefix="test")
print(f"Test CER (IAM dataset subset): {results['test_cer']*100:.2f}%")
print(f"Test WER (IAM dataset subset): {results['test_wer']*100:.2f}%")

# Check if the target CER and WER are met
if results['test_cer'] * 100 <= 7 and results['test_wer'] * 100 <= 15:
    print("Target achieved: CER ≤ 7% and WER ≤ 15%")
else:
    print("Target not achieved. Adjust training or preprocessing.")