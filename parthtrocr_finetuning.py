# -*- coding: utf-8 -*-
"""TROCR Finetuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/richasin/trocr-finetuning.107dc560-68b2-499a-a1c3-6af1ac1b6a9d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250418/auto/storage/goog4_request%26X-Goog-Date%3D20250418T120135Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9a34a5f346911d88fdb14a2332ee9827b476d69a8b0dbc235c2a05f48b047345cda41e434a0448143f4280eed4e71ccc17cee97a28938b4b6a9019c867bcf44512cec34fdc086d404e3377a6ad2d01d2a20baf1e2620a18c703a272aa4dbd2e1ce9d5a6758494b00a0d106c2e2365cb2e5cf05562faaf4ca409fa2d5464a9af8d65350737715dab040ec6afe660398518f79b502cba3b8dc7d72ec554075e19413ca8d95871f5bbbd3121768bee4aa7109ee1207371df79884a65ed99993d0d5da06636fccb6b7bdf5964b781db90bc96f32b98f4e70eb57e888a59e8c056d1735c6b89230936bd7effb3d65c91abff0968c8d4340ace1dd90692c02b434f4be
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
andrianang_imgur5k_sample_path = kagglehub.dataset_download('andrianang/imgur5k-sample')
hongvnphi_iam_handwriting_preprocessed_path = kagglehub.dataset_download('hongvnphi/iam-handwriting-preprocessed')

print('Data source import complete.')

!pip install numpy
!pip install pandas
!pip install Pillow
!pip install torch torchvision
!pip install transformers
!pip install datasets
!pip install evaluate
!pip install jiwer
!pip install wandb

import os
import wandb

# Set wandb API key (replace with your actual key)
os.environ["WANDB_API_KEY"] = "89470e3cb36c534d5103f578c9667a4c0c8131b4"
wandb.login(key=os.environ["WANDB_API_KEY"])

import os
import pandas as pd
from sklearn.model_selection import train_test_split

# -------------------------------
# PART 1: PROCESS IMGUR5K SAMPLE DATASET
# -------------------------------
def load_imgur_labels(labels_file, images_dir, sep="\t"):
    """
    Reads a labels file where each line contains a relative image path and its transcription.
    Returns a DataFrame with columns: image_filename, text, image_path.
    Expected format:
      ./eval_images\image_0.png It
      ./eval_images\image_1.png takes
      ...
    """
    data = []
    with open(labels_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    for line in lines:
        line = line.strip()
        if not line:
            continue
        tokens = line.split()
        if len(tokens) < 2:
            continue
        # Normalize path
        rel_path = tokens[0].replace("\\", "/")
        filename = os.path.basename(rel_path)
        image_path = os.path.join(images_dir, filename)
        text = " ".join(tokens[1:])
        data.append({"image_filename": filename, "text": text, "image_path": image_path})
    return pd.DataFrame(data)

# Base path for IMGUR5K sample dataset on Kaggle
imgur_base_path = "/kaggle/input/imgur5k-sample/"
train_images_dir = os.path.join(imgur_base_path, "train_images")
test_images_dir  = os.path.join(imgur_base_path, "test_images")
eval_images_dir  = os.path.join(imgur_base_path, "eval_images")

train_labels_file = os.path.join(imgur_base_path, "train_labels.txt")
test_labels_file  = os.path.join(imgur_base_path, "test_labels.txt")
eval_labels_file  = os.path.join(imgur_base_path, "eval_labels.txt")

# Load IMGUR5K data
df_imgur_train = load_imgur_labels(train_labels_file, train_images_dir)
df_imgur_test  = load_imgur_labels(test_labels_file, test_images_dir)
df_imgur_eval  = load_imgur_labels(eval_labels_file, eval_images_dir)

print("IMGUR5K splits:")
print("Train:", df_imgur_train.shape)
print("Test:", df_imgur_test.shape)
print("Eval:", df_imgur_eval.shape)

# -------------------------------
# PART 2: PROCESS IAM HANDWRITING PREPROCESSED DATASET
# -------------------------------
import kagglehub
iam_path = kagglehub.dataset_download("hongvnphi/iam-handwriting-preprocessed")
# Adjust the IAM path if needed:
iam_path = iam_path + '/kaggle/working/iam-handwriting-dataset/'
print("Path to IAM dataset files:", iam_path)

def load_iam_preprocessed(split_folder):
    """
    Reads image and label files from a split folder.
    Expects 'images' and 'labels' subfolders.
    Returns a DataFrame with columns: image_filename, text, image_path.
    """
    images_dir = os.path.join(split_folder, "images")
    labels_dir = os.path.join(split_folder, "labels")
    label_files = [f for f in os.listdir(labels_dir) if f.endswith(".txt")]
    data = []
    for label_file in label_files:
        base = os.path.splitext(label_file)[0]
        image_filename = base + ".png"
        image_path = os.path.join(images_dir, image_filename)
        label_path = os.path.join(labels_dir, label_file)
        with open(label_path, "r", encoding="utf-8") as f:
            label_text = f.read().strip()
        data.append({
            "image_filename": image_filename,
            "text": label_text,
            "image_path": image_path
        })
    return pd.DataFrame(data)

iam_dataset_path = iam_path
df_iam_train = load_iam_preprocessed(os.path.join(iam_dataset_path, "train"))
df_iam_val   = load_iam_preprocessed(os.path.join(iam_dataset_path, "validation"))
df_iam_test  = load_iam_preprocessed(os.path.join(iam_dataset_path, "test"))

print("IAM splits:")
print("Train:", df_iam_train.shape)
print("Validation:", df_iam_val.shape)
print("Test:", df_iam_test.shape)

# -------------------------------
# PART 3: COMBINE ALL DATA SOURCES AND SPLIT INTO TRAIN, VAL, TEST
# -------------------------------
# Combine all splits from both datasets
df_imgur_all = pd.concat([df_imgur_train, df_imgur_test, df_imgur_eval], ignore_index=True)
df_iam_all   = pd.concat([df_iam_train, df_iam_val, df_iam_test], ignore_index=True)
df_all = pd.concat([df_imgur_all, df_iam_all], ignore_index=True)

print("Total combined data shape:", df_all.shape)

# Shuffle the combined data
df_all = df_all.sample(frac=1, random_state=42).reset_index(drop=True)

# Split the combined data: 80% train, 10% validation, 10% test
df_train, temp = train_test_split(df_all, test_size=0.2, random_state=42)
df_val, df_test = train_test_split(temp, test_size=0.5, random_state=42)

print("Final split shapes:")
print("Train:", df_train.shape)
print("Validation:", df_val.shape)
print("Test:", df_test.shape)

# -------------------------------
# PART 4: SAVE THE SPLITS AS CSV FILES
# -------------------------------
df_train.to_csv("combined_train.csv", index=False)
df_val.to_csv("combined_val.csv", index=False)
df_test.to_csv("combined_test.csv", index=False)
print("Saved CSV files: combined_train.csv, combined_val.csv, combined_test.csv")

# Import necessary libraries
import os
import random
import numpy as np
import pandas as pd
from PIL import Image
import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms
from transformers import (
    VisionEncoderDecoderModel,
    TrOCRProcessor,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    default_data_collator,
    TrainerCallback,
    EarlyStoppingCallback
)
import evaluate

# Set seed for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(44)

# ===============================
# CONFIGURATION PARAMETERS
# ===============================
class Config:
    BATCH_SIZE = 4                 # Adjust based on your hardware
    EPOCHS = 5                     # Number of training epochs
    LEARNING_RATE = 5e-5           # Optimizer learning rate
    TRAIN_CSV = "combined_train.csv"
    VAL_CSV = "combined_val.csv"
    TEST_CSV = "combined_test.csv"
    BASE_IMAGE_DIR = ""            # If image paths in CSV are relative, specify the base directory
    TRAIN_SIZE = 12000             # Optional: limit number of training samples
    VAL_SIZE = 1800                # Optional: limit number of validation samples

# ===============================
# CUSTOM DATASET DEFINITION
# ===============================
class OCRDataset(Dataset):
    def __init__(self, csv_file, processor, transform=None, max_samples=None):
        self.data = pd.read_csv(csv_file)
        if max_samples is not None:
            self.data = self.data.head(max_samples)
        # Ensure only valid rows with non-null image paths are kept
        self.data = self.data[self.data["image_path"].notnull() & (self.data["image_path"] != "")]
        self.processor = processor
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        # If a base directory is specified, combine it with the CSV image path
        image_path = os.path.join(Config.BASE_IMAGE_DIR, row["image_path"]) if Config.BASE_IMAGE_DIR else row["image_path"]
        image = Image.open(image_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        text = str(row["text"])
        # Return raw image and text; actual processing is done in the collator.
        return {"image": image, "text": text}

# ===============================
# DATA TRANSFORMATIONS
# ===============================
train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ColorJitter(brightness=0.5, hue=0.3),
    transforms.RandomRotation(degrees=10),
    transforms.ToTensor(),
])
val_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# ===============================
# LOAD PRE-TRAINED MODEL AND PROCESSOR
# ===============================
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-large-handwritten")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-large-handwritten")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# If multiple GPUs are available, use DataParallel.
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs with DataParallel.")
    model = torch.nn.DataParallel(model)

# Set special tokens if not already defined
if model.config.decoder_start_token_id is None:
    model.config.decoder_start_token_id = processor.tokenizer.bos_token_id
if model.config.pad_token_id is None:
    model.config.pad_token_id = processor.tokenizer.pad_token_id

# ===============================
# CREATE DATASETS
# ===============================
train_dataset = OCRDataset(Config.TRAIN_CSV, processor, transform=train_transforms, max_samples=Config.TRAIN_SIZE)
val_dataset = OCRDataset(Config.VAL_CSV, processor, transform=val_transforms, max_samples=Config.VAL_SIZE)
test_dataset = OCRDataset(Config.TEST_CSV, processor, transform=val_transforms)

# ===============================
# DATA COLLATOR FUNCTION
# ===============================
def data_collator(features):
    # Filter out any samples missing required keys (just in case)
    valid_features = [f for f in features if "image" in f and "text" in f]
    if len(valid_features) < len(features):
        print("Warning: Filtering out some samples missing required keys.")
    if not valid_features:
        raise ValueError("All samples in the batch are invalid.")

    images = [f["image"] for f in valid_features]
    texts  = [f["text"] for f in valid_features]

    # Process images and texts using the processor in batch
    pixel_values = processor(images=images, return_tensors="pt", do_rescale=False).pixel_values
    labels = processor.tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    ).input_ids
    # Replace padding token ids with -100 so that they are ignored by the loss function
    labels[labels == processor.tokenizer.pad_token_id] = -100

    return {"pixel_values": pixel_values, "labels": labels}

# ===============================
# EVALUATION METRICS
# ===============================
cer_metric = evaluate.load("cer")
wer_metric = evaluate.load("wer")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Ensure that -100 values are replaced with the pad token id for decoding
    predictions = np.where(predictions == -100, processor.tokenizer.pad_token_id, predictions)
    labels = np.where(labels == -100, processor.tokenizer.pad_token_id, labels)

    pred_str = processor.batch_decode(predictions, skip_special_tokens=True)
    label_str = processor.batch_decode(labels, skip_special_tokens=True)

    cer = cer_metric.compute(predictions=pred_str, references=label_str)
    wer = wer_metric.compute(predictions=pred_str, references=label_str)

    return {"cer": cer, "wer": wer}

# ===============================
# CUSTOM TRAINER CALLBACK FOR LOGGING
# ===============================
class LoggingCallback(TrainerCallback):
    def on_train_begin(self, args, state, control, **kwargs):
        print("Training is starting...")

    def on_epoch_begin(self, args, state, control, **kwargs):
        print(f"Epoch {state.epoch:.2f} is starting...")

    def on_epoch_end(self, args, state, control, **kwargs):
        if state.log_history:
            print(f"Epoch {state.epoch:.2f} finished. Latest metrics: {state.log_history[-1]}")
        else:
            print(f"Epoch {state.epoch:.2f} finished.")

    def on_train_end(self, args, state, control, **kwargs):
        print("Training has finished.")

# ===============================
# TRAINING ARGUMENTS AND TRAINER SETUP
# ===============================
training_args = Seq2SeqTrainingArguments(
    output_dir="./trocr_finetuned",
    per_device_train_batch_size=Config.BATCH_SIZE,
    per_device_eval_batch_size=Config.BATCH_SIZE,
    num_train_epochs=Config.EPOCHS,
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=Config.LEARNING_RATE,
    fp16=True,
    logging_steps=50,
    logging_first_step=True,
    log_level="info",
    save_total_limit=2,
    predict_with_generate=True,
    dataloader_num_workers=4,
    report_to=["wandb"],            # Change or remove if you aren't using Weights & Biases
    logging_dir="./logs",
    run_name="trocr_finetuning_experiment",
    remove_unused_columns=False,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Add callbacks for additional logging and early stopping
trainer.add_callback(LoggingCallback)
trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))

# ===============================
# TRAIN THE MODEL
# ===============================
trainer.train()

# Save the fine-tuned model and processor
trainer.save_model("./trocr_finetuned_final")
processor.save_pretrained("./trocr_finetuned_final")

# ===============================
# DEFINE INFERENCE FUNCTION FOR OCR
# ===============================
def ocr_inference(image_path):
    image = Image.open(image_path).convert("RGB")
    image = val_transforms(image)
    pixel_values = processor(images=image, return_tensors="pt", do_rescale=False).pixel_values.to(device)
    generated_ids = model.generate(pixel_values)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return generated_text

# ===============================
# SAMPLE INFERENCE
# ===============================
sample_image_path = pd.read_csv(Config.TRAIN_CSV).iloc[0]["image_path"]
print("OCR Output:", ocr_inference(sample_image_path))

# ===============================
# INFERENCE ON TEST SET: DISPLAY IMAGES AND PRINT OCR OUTPUTS
# ===============================
from IPython.display import display  # For inline image display in notebooks

# Load test CSV data
test_df = pd.read_csv(Config.TEST_CSV)
print("Performing inference on the test set...")

# Loop through a subset of test samples (change range as needed)
for idx, row in test_df.iterrows():
    image_path = row["image_path"]
    try:
        # Open and display the test image
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"Error loading image at {image_path}: {e}")
        continue

    print(f"\nTest Image {idx}:")
    display(image)  # This will display the image inline in a Jupyter Notebook
    # Run OCR inference on the test image
    output_text = ocr_inference(image_path)
    print("OCR Output:", output_text)

    # Limit output to a few examples; remove or adjust this condition for full test set inference.
    if idx >= 4:  # For example, show only first 5 test images.
        break

test_metrics = trainer.evaluate(test_dataset)
print("Test Metrics:", test_metrics)

# Define target metrics for evaluation
target_cer = 0.07  # 7% target CER
target_wer = 0.15  # 15% target WER

if test_metrics["eval_cer"] <= target_cer and test_metrics["eval_wer"] <= target_wer:
    print("Target metrics achieved: CER <= 7% and WER <= 15%")
else:
    print(f"Target metrics not achieved. CER: {test_metrics['eval_cer']:.4f}, WER: {test_metrics['eval_wer']:.4f}")

# ===============================
# EVALUATE THE MODEL ON TEST DATASET
# ===============================
test_metrics = trainer.evaluate(test_dataset)
print("Test Metrics:", test_metrics)

# Define target metrics for evaluation
target_cer = 0.07  # 7% target CER
target_wer = 0.15  # 15% target WER

if test_metrics["eval_cer"] <= target_cer and test_metrics["eval_wer"] <= target_wer:
    print("Target metrics achieved: CER <= 7% and WER <= 15%")
else:
    print(f"Target metrics not achieved. CER: {test_metrics['eval_cer']:.4f}, WER: {test_metrics['eval_wer']:.4f}")



